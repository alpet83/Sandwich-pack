<python src="/spack.py" mod_time="2025-07-21 15:41:30Z">
# /spack.py, updated 2025-07-15 09:57 EEST
import os
import datetime
import logging
import argparse
from pathlib import Path
from lib.sandwich_pack import SandwichPack

logging.basicConfig(level=logging.DEBUG, format='%(asctime)s #%(levelname)s: %(message)s')

def get_file_mod_time(file_path):
    mtime = os.path.getmtime(file_path)
    mod_time = datetime.datetime.fromtimestamp(mtime, datetime.UTC)
    return mod_time.strftime("%Y-%m-%d %H:%M:%SZ")

def is_hidden_file(filepath):
    return any(part.startswith(".") for part in filepath.parts)

def collect_files(root_dir):
    content = []
    root_path = Path(root_dir).parent
    logging.debug(f"Scanning directory: {root_dir}")
    if not os.path.exists(root_dir):
        logging.error(f"Directory {root_dir} does not exist")
        return content
    for file_path in Path(root_dir).rglob("*"):
        if file_path.is_file() and not is_hidden_file(file_path):
            relative_path = f"/{file_path.relative_to(root_path)}".replace("\\", "/")
            extension = Path(file_path).suffix.lower()
            content_type = extension if extension else ""
            if not content_type or not SandwichPack.supported_type(content_type):
                logging.debug(f"Skipping unsupported content_type: {content_type} for {relative_path}")
                continue
            try:
                with open(file_path, "r", encoding="utf-8-sig", errors="replace") as f:
                    text = f.read()
                logging.debug(f"Read {file_path} with encoding: utf-8-sig")
            except UnicodeDecodeError as e:
                logging.warning(f"Non-UTF-8 characters in {file_path}, replaced with �: {e}")
                continue
            mod_time = get_file_mod_time(file_path)
            logging.debug(f"Collected file: {relative_path} with content_type: {content_type}")
            content.append(SandwichPack.create_block(
                content_text=text,
                content_type=content_type,
                file_name=relative_path,
                timestamp=mod_time
            ))
    for file_path in root_path.glob("*.toml"):
        if not is_hidden_file(file_path):
            relative_path = f"/{file_path.name}".replace("\\", "/")
            content_type = ".toml"
            if not SandwichPack.supported_type(content_type):
                logging.debug(f"Skipping unsupported content_type: {content_type} for {relative_path}")
                continue
            try:
                with open(file_path, "r", encoding="utf-8-sig", errors="replace") as f:
                    text = f.read()
                logging.debug(f"Read {file_path} with encoding: utf-8-sig")
            except UnicodeDecodeError as e:
                logging.warning(f"Non-UTF-8 characters in {file_path}, replaced with �: {e}")
                continue
            mod_time = get_file_mod_time(file_path)
            logging.debug(f"Collected file: {relative_path} with content_type: {content_type}")
            content.append(SandwichPack.create_block(
                content_text=text,
                content_type=content_type,
                file_name=relative_path,
                timestamp=mod_time
            ))
    return content

def main():
    logging.info("Starting spack CLI")
    SandwichPack.load_block_classes()
    project_dir = "."
    output_dir = "./sandwiches"
    files_content = collect_files(project_dir)
    if not files_content:
        logging.error("No files collected, exiting")
        raise SystemExit("Error: No files found in the specified directory")
    logging.info(f"Collected {len(files_content)} files")
    parser = argparse.ArgumentParser(
        prog='Sandwich Packer',
        description='Combining all project sources files into sandwich structured several text files',
        epilog='Best for using with chatbots like Grok or ChatGPT')

    parser.add_argument('project_name')
    args = parser.parse_args()
    packer = SandwichPack(args.project_name, max_size=80_000)
    result = packer.pack(files_content)
    os.makedirs(output_dir, exist_ok=True)
    for i, sandwich in enumerate(result["sandwiches"], 1):
        output_file = Path(output_dir) / f"sandwich_{i}.txt"
        with open(output_file, "w", encoding="utf-8") as f:
            f.write(sandwich)
        logging.info(f"Created {output_file} ({len(sandwich.encode('utf-8'))} bytes)")
    global_index_file = Path(output_dir) / "sandwiches_index.json"
    with open(global_index_file, "w", encoding="utf-8") as f:
        f.write(result["index"])
    global_index_file = Path(output_dir) / "sandwiches_structure.json"
    with open(global_index_file, "w", encoding="utf-8") as f:
        f.write(result["deep_index"])

    logging.info(f"Created {global_index_file}")

if __name__ == "__main__":
    main()

</python>
<python src="/lib/content_block.py" mod_time="2025-07-15 13:47:39Z">
# /lib/content_block.py, updated 2025-07-15 15:43 EEST
import logging
from typing import Optional


class ContentBlock:
    supported_types = [':document', ':post']

    def __init__(self, content_text: str, content_type: str, file_name: Optional[str] = None, timestamp: Optional[str] = None, **kwargs):
        self.content_text = content_text
        self.content_type = content_type
        self.tag = "post" if content_type == ":post" else "document"
        self.file_name = file_name
        self.timestamp = timestamp
        self.post_id = kwargs.get('post_id')
        self.user_id = kwargs.get('user_id')
        self.relevance = kwargs.get('relevance', 0)
        self.file_id = kwargs.get('file_id')
        self.tokens = self.estimate_tokens(content_text)
        logging.debug(f"Initialized ContentBlock with content_type={content_type}, tag={self.tag}, file_name={file_name}")

    def estimate_tokens(self, content: str) -> int:
        return len(content) // 4

    def parse_content(self):
        return {
            "entities": [],
            "dependencies": {
                "imports": [],
                "modules": [],
                "calls": []
            }
        }

    def to_sandwich_block(self):
        attrs = []
        if self.content_type == ":post":
            attrs.append(f'post_id="{self.post_id}"')
            if self.user_id is not None:
                attrs.append(f'user_id="{self.user_id}"')
            if self.timestamp:
                attrs.append(f'mod_time="{self.timestamp}"')
            if self.relevance is not None:
                attrs.append(f'relevance="{self.relevance}"')
        else:
            if self.file_name:
                attrs.append(f'src="{self.file_name}"')
            if self.timestamp:
                attrs.append(f'mod_time="{self.timestamp}"')
            if self.file_id is not None:
                attrs.append(f'file_id="{self.file_id}"')
        attr_str = " ".join(attrs)
        return f"<{self.tag} {attr_str}>\n{self.content_text}\n</{self.tag}>"
</python>
<python src="/lib/document_block.py" mod_time="2025-07-15 12:56:33Z">
# /lib/document_block.py, created 2025-07-15 15:53 EEST
import logging
from typing import Dict
from lib.content_block import ContentBlock
from lib.sandwich_pack import SandwichPack

logging.basicConfig(level=logging.DEBUG, format='%(asctime)s #%(levelname)s: %(message)s')

class DocumentBlock(ContentBlock):
    supported_types = ['.md', '.conf', '.toml', '.rulz']

    def __init__(self, content_text: str, content_type: str, file_name: str, timestamp: str, **kwargs):
        super().__init__(content_text, content_type, file_name, timestamp, **kwargs)
        self.tag = {
            '.md': 'markdown',
            '.conf': 'conf',
            '.toml': 'toml',
            '.rulz': 'rules'
        }.get(content_type, 'document')
        logging.debug(f"Initialized DocumentBlock with tag={self.tag}, content_type={content_type}, file_name={file_name}")

    def parse_content(self) -> Dict:
        return {
            "entities": [],
            "dependencies": {
                "imports": [],
                "modules": [],
                "calls": []
            }
        }

SandwichPack.register_block_class(DocumentBlock)
</python>
<python src="/lib/js_block.py" mod_time="2025-07-15 13:00:12Z">
# /lib/js_block.py, updated 2025-07-15 15:43 EEST
import re
import logging
from typing import Dict
from lib.content_block import ContentBlock
from lib.sandwich_pack import SandwichPack

class ContentCodeJs(ContentBlock):
    supported_types = [".js"]

    def __init__(self, content_text: str, content_type: str, file_name: str, timestamp: str, **kwargs):
        super().__init__(content_text, content_type, file_name, timestamp, **kwargs)
        self.tag = "jss"
        logging.debug(f"Initialized ContentCodeJs with tag={self.tag}, file_name={file_name}")

    def parse_content(self) -> Dict:
        entities = []
        dependencies = {"modules": [], "imports": [], "calls": []}
        fn_pattern = re.compile(r"(?:function\s+|const\s+\w+\s*=\s*(?:async\s+)?function\s*|const\s+\w+\s*=\s*\([^)]*\)\s*=>)\s*(\w+)\s*\(", re.DOTALL | re.MULTILINE)
        for match in fn_pattern.finditer(self.content_text):
            full_text = self._extract_full_entity(match.start(), match.end())
            entities.append({"type": "function", "name": match.group(1), "visibility": "public", "tokens": self._estimate_tokens(full_text)})
        import_pattern = re.compile(r"import\s+{?([\w,\s]+)}?\s+from\s+['\"]([^'\"]+)['\"]|require\s*\(['\"]([^'\"]+)['\"]\)", re.MULTILINE)
        for match in import_pattern.finditer(self.content_text):
            items = [item.strip() for item in match.group(1).split(",")] if match.group(1) else []
            module = match.group(2) or match.group(3)
            for item in items:
                dependencies["imports"].append(item)
            if module:
                dependencies["modules"].append(module)
        return {"entities": entities, "dependencies": {k: sorted(list(set(v))) for k, v in dependencies.items()}}

    def _extract_full_entity(self, start: int, end_header: int) -> str:
        brace_count = 1
        i = end_header
        while i < len(self.content_text) and brace_count > 0:
            if self.content_text[i] == '{':
                brace_count += 1
            elif self.content_text[i] == '}':
                brace_count -= 1
            i += 1
        return self.content_text[start:i] if brace_count == 0 else self.content_text[start:end_header]

    def _estimate_tokens(self, content: str) -> int:
        return len(content) // 4

SandwichPack.register_block_class(ContentCodeJs)
</python>
<python src="/lib/python_block.py" mod_time="2025-07-15 13:00:05Z">
# /lib/python_block.py, updated 2025-07-15 15:43 EEST
import re
import logging
from typing import Dict
from lib.content_block import ContentBlock
from lib.sandwich_pack import SandwichPack

class ContentCodePython(ContentBlock):
    supported_types = [".py"]

    def __init__(self, content_text: str, content_type: str, file_name: str, timestamp: str, **kwargs):
        super().__init__(content_text, content_type, file_name, timestamp, **kwargs)
        self.tag = "python"
        logging.debug(f"Initialized ContentCodePython with tag={self.tag}, file_name={file_name}")

    def parse_content(self) -> Dict:
        entities = []
        dependencies = {"modules": [], "imports": [], "calls": []}
        fn_pattern = re.compile(r"(?P<vis>@classmethod\s+|@staticmethod\s+)?def\s+(?P<name>\w+)\s*\(", re.DOTALL | re.MULTILINE)
        for match in fn_pattern.finditer(self.content_text):
            full_text = self._extract_full_entity(match.start(), match.end())
            vis = "public" if match.group('vis') else "private"
            entities.append({"type": "function", "name": match.group('name'), "visibility": vis, "tokens": self._estimate_tokens(full_text)})
        class_pattern = re.compile(r"(?P<vis>@classmethod\s+|@staticmethod\s+)?class\s+(?P<name>\w+)\s*(?:\([^)]*\))?\s*:", re.DOTALL | re.MULTILINE)
        for match in class_pattern.finditer(self.content_text):
            full_text = self._extract_full_entity(match.start(), match.end())
            vis = "public" if match.group('vis') else "private"
            entities.append({"type": "class", "name": match.group('name'), "visibility": vis, "tokens": self._estimate_tokens(full_text)})
        import_pattern = re.compile(r"from\s+([\w.]+)\s+import\s+([\w,\s]+)", re.MULTILINE)
        for match in import_pattern.finditer(self.content_text):
            items = [item.strip() for item in match.group(2).split(",")]
            for item in items:
                dependencies["imports"].append(item)
            dependencies["modules"].append(match.group(1))
        return {"entities": entities, "dependencies": {k: sorted(list(set(v))) for k, v in dependencies.items()}}

    def _extract_full_entity(self, start: int, end_header: int) -> str:
        indent_level = 0
        i = end_header
        while i < len(self.content_text):
            if self.content_text[i] == ':':
                indent_level += 1
                i += 1
                while i < len(self.content_text) and self.content_text[i].isspace():
                    i += 1
                while i < len(self.content_text) and indent_level > 0:
                    line = self.content_text[i:].split('\n', 1)[0]
                    if not line.strip() or line.strip().startswith('#'):
                        i += len(line) + 1
                        continue
                    indent = len(line) - len(line.lstrip())
                    if indent == 0 and line.strip():
                        break
                    i += len(line) + 1
                return self.content_text[start:i]
            i += 1
        return self.content_text[start:end_header]

    def _estimate_tokens(self, content: str) -> int:
        return len(content) // 4

SandwichPack.register_block_class(ContentCodePython)
</python>
<python src="/lib/rust_block.py" mod_time="2025-07-15 12:59:54Z">
# /lib/rust_block.py, updated 2025-07-15 15:43 EEST
import re
import os
import logging
from typing import Dict
from lib.content_block import ContentBlock
from lib.sandwich_pack import SandwichPack

class ContentCodeRust(ContentBlock):
    supported_types = [".rs"]

    def __init__(self, content_text: str, content_type: str, file_name: str, timestamp: str, **kwargs):
        super().__init__(content_text, content_type, file_name, timestamp, **kwargs)
        self.tag = "rustc"
        logging.debug(f"Initialized ContentCodeRust with tag={self.tag}, file_name={file_name}")

    def parse_content(self) -> Dict:
        entities = []
        dependencies = {"modules": [], "imports": [], "calls": []}
        struct_pattern = re.compile(r"(?P<vis>pub\s+)?struct\s+(?P<name>\w+)(<.*?>)?\s*{", re.DOTALL | re.MULTILINE)
        for match in struct_pattern.finditer(self.content_text):
            full_text = self._extract_full_entity(match.start(), match.end())
            vis = "public" if match.group('vis') else "private"
            entities.append({"type": "struct", "name": match.group('name'), "visibility": vis, "tokens": self._estimate_tokens(full_text)})
        trait_pattern = re.compile(r"(?P<vis>pub\s+)?trait\s+(?P<name>\w+)(<.*?>)?\s*{", re.DOTALL | re.MULTILINE)
        for match in trait_pattern.finditer(self.content_text):
            full_text = self._extract_full_entity(match.start(), match.end())
            vis = "public" if match.group('vis') else "private"
            entities.append({"type": "trait", "name": match.group('name'), "visibility": vis, "tokens": self._estimate_tokens(full_text)})
        fn_pattern = re.compile(r"(?P<vis>pub\s+)?(?:async\s+)?fn\s+(?P<name>\w+)\s*\((.*?)\)\s*(->\s*[^ {]+)?\s*{", re.DOTALL | re.MULTILINE)
        for match in fn_pattern.finditer(self.content_text):
            full_text = self._extract_full_entity(match.start(), match.end())
            vis = "public" if match.group('vis') else "private"
            entities.append({"type": "function", "name": match.group('name'), "visibility": vis, "tokens": self._estimate_tokens(full_text)})
        module_pattern = re.compile(r"pub\s+mod\s+(\w+);")
        for match in module_pattern.finditer(self.content_text):
            module_name = match.group(1)
            module_path = f"{os.path.dirname(self.file_name)}/{module_name}/mod.rs".replace("\\", "/")
            if not module_path.startswith("/"):
                module_path = f"/{module_path}"
            dependencies["modules"].append(module_name)
        import_pattern = re.compile(r"use\s+crate::([\w:]+)(?:\{([\w,: ]+)\})?;")
        for match in import_pattern.finditer(self.content_text):
            path = match.group(1).replace("::", "/")
            if match.group(2):
                for item in match.group(2).split(","):
                    dependencies["imports"].append(item.strip())
            else:
                dependencies["imports"].append(path.split("/")[-1])
        call_pattern = re.compile(r"\b(\w+)\s*\(")
        for match in call_pattern.finditer(self.content_text):
            dependencies["calls"].append(match.group(1))
        return {"entities": entities, "dependencies": {k: sorted(list(set(v))) for k, v in dependencies.items()}}

    def _extract_full_entity(self, start: int, end_header: int) -> str:
        brace_count = 1
        i = end_header
        while i < len(self.content_text) and brace_count > 0:
            if self.content_text[i] == '{':
                brace_count += 1
            elif self.content_text[i] == '}':
                brace_count -= 1
            i += 1
        return self.content_text[start:i] if brace_count == 0 else self.content_text[start:end_header]

    def _estimate_tokens(self, content: str) -> int:
        return len(content) // 4

SandwichPack.register_block_class(ContentCodeRust)
</python>
<python src="/lib/sandwich_pack.py" mod_time="2025-07-21 16:25:00Z">
# /lib/sandwich_pack.py, updated 2025-07-16 14:33 EEST
import hashlib
import importlib.util
import os
import re
import logging
import datetime
import json
import math
import traceback
from pathlib import Path
from typing import List, Dict, Optional
from .content_block import ContentBlock


def compute_md5(content: str) -> str:
    return hashlib.md5(content.encode("utf-8")).hexdigest()


def estimate_tokens(content: str) -> int:
    """Estimates tokens by counting words and spaces more accurately."""
    if not content:
        return 0
    # Split content into words and spaces/newlines
    tokens = 0
    words = re.findall(r'\S+', content)
    for word in words:
        if len(word) >= 5:
            tokens += math.ceil(len(word) / 4)
        else:
            tokens += 1
    # Count spaces and newlines as single tokens
    spaces = len(re.findall(r'\s+', content))
    tokens += spaces
    logging.debug("Estimated tokens for content (length=%d): %d tokens (words=%d, spaces=%d)", len(content), tokens, len(words), spaces)
    return tokens

class SandwichPack:
    _block_classes = []

    def __init__(self, project_name: str, max_size: int = 80_000, system_prompt: Optional[str] = None):
        self.project_name = project_name
        self.max_size = max_size
        self.system_prompt = system_prompt
        self.datasheet = {
            "project_root": "/app",
            "backend_address": "http://localhost:8080",
            "frontend_address": "http://vps.vpn:8008",
            "resources": {
                "database": "sqlite:///app/data/multichat.db",
                "log_file": "/app/logs/colloquium_core.log"
            }
        }
        self.busy_ids = set()

    @classmethod
    def register_block_class(cls, block_class):
        logging.debug(f"Registering block class: {block_class.__name__}")
        cls._block_classes.append(block_class)

    @classmethod
    def load_block_classes(cls):
        for module in Path(__file__).parent.glob("*_block.py"):
            module_name = module.stem
            try:
                spec = importlib.util.spec_from_file_location(module_name, module)
                mod = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(mod)
                logging.debug(f"Loaded module: {module_name}")
            except Exception as e:
                logging.error(f"Failed to load module {module_name}: {str(e)}")

    @classmethod
    def supported_type(cls, content_type: str) -> bool:
        for block_class in cls._block_classes:
            if content_type in block_class.supported_types:
                logging.debug(f"Supported content_type={content_type} by {block_class.__name__}")
                return True
        logging.debug(f"Content_type={content_type} falls back to ContentBlock")
        return content_type in ContentBlock.supported_types

    @classmethod
    def create_block(cls, content_text: str, content_type: str, file_name: Optional[str] = None, timestamp: Optional[str] = None, **kwargs) -> ContentBlock:
        for block_class in cls._block_classes:
            if content_type in block_class.supported_types:
                logging.debug(f"Creating block with {block_class.__name__} for content_type={content_type}")
                return block_class(content_text, content_type, file_name, timestamp, **kwargs)
        logging.debug(f"Creating default ContentBlock for content_type={content_type}")
        return ContentBlock(content_text, content_type, file_name, timestamp, **kwargs)


    def generate_unique_file_id(self) -> int:
        file_id = 0
        while file_id in self.busy_ids:
            file_id += 1
        self.busy_ids.add(file_id)
        logging.debug(f"Generated unique file_id={file_id}")
        return file_id


    def pack(self, blocks: List[ContentBlock], users: List[Dict] = None) -> Dict[str, any]:
        try:
            self.busy_ids.clear()
            file_map = {}
            file_list = []
            entity_map = {}
            entities_list = []
            name_to_locations = {}

            for block in blocks:
                if block.content_type != ":post" and block.file_id is not None:
                    self.busy_ids.add(block.file_id)

            for block in blocks:
                if block.content_type != ":post" and block.file_name:
                    file_id = block.file_id if block.file_id is not None else self.generate_unique_file_id()
                    file_map[block.file_name] = file_id
                    file_list.append(f"{file_id},{block.file_name},{compute_md5(block.to_sandwich_block())},{block.tokens},{block.timestamp}")
                parsed = block.parse_content()
                if block.file_name and parsed["entities"]:
                    for ent in parsed["entities"]:
                        key = (block.file_name, ent["type"], ent["name"])
                        if key not in entity_map:
                            entity_map[key] = len(entities_list)
                            vis_short = "pub" if ent["visibility"] == "public" else "prv"
                            entities_list.append(f"{vis_short},{ent['type']},{ent['name']},{ent['tokens']}")
                            name_to_locations.setdefault(ent["name"], []).append((block.file_name, ent["type"]))

            current_size = 0
            current_tokens = 0
            current_content = []
            current_index = []
            current_file_index = 1
            sandwiches = []
            global_index = {
                "packer_version": "0.3.0",
                "context_date": datetime.datetime.utcnow().strftime("%Y-%m-%d %H:%M:%SZ"),
                "templates": {
                    "filelist": "file_id,file_name,md5,tokens,timestamp",
                    "users": "user_id,username,role"
                },
                "project_name": self.project_name,
                "datasheet": self.datasheet,
                "files": file_list,
                "users": users or []
            }

            deep_index = {
                "templates": {
                    "entities": "vis(pub/prv),type,name,tokens",
                },
                "dep_format": "modules: details[:file_id] (str); imports: index (int) or details[:file_id] (str); calls: index (int)",
                "entities": entities_list,
                "sandwiches": []
            }


            current_line = 1
            for block in blocks:
                parsed = block.parse_content()
                block_str = block.to_sandwich_block()
                block_size = len(block_str.encode("utf-8"))
                block_tokens = block.tokens
                block_lines = block_str.count("\n") + 1

                if current_size + block_size > self.max_size or current_tokens + block_tokens > 20_000:
                    sandwiches.append("".join(current_content))
                    deep_index["sandwiches"].append({
                        "file": f"sandwich_{current_file_index}.txt",
                        "blocks": current_index
                    })
                    current_file_index += 1
                    current_content = []
                    current_index = []
                    current_size = 0
                    current_tokens = 0
                    current_line = 1

                block_data = {
                    "start_line": current_line
                }
                if block.content_type == ":post":
                    block_data["post_id"] = block.post_id
                else:
                    block_data["file_id"] = block.file_id if block.file_id is not None else file_map.get(block.file_name)
                deps = parsed["dependencies"]
                if deps["imports"]:
                    block_data["imports"] = deps["imports"]
                if deps["modules"]:
                    block_data["modules"] = deps["modules"]
                if deps["calls"]:
                    block_data["calls"] = deps["calls"]
                if block.file_name and parsed["entities"]:
                    ent_uids = [entity_map[(block.file_name, e["type"], e["name"])] for e in parsed["entities"]]
                    block_data["entities"] = sorted(ent_uids)
                current_content.append(block_str + "\n")
                current_index.append(block_data)
                current_size += block_size
                current_tokens += block_tokens
                current_line += block_lines

            if current_content:
                sandwiches.append("".join(current_content))
                deep_index["sandwiches"].append({
                    "file": f"sandwich_{current_file_index}.txt",
                    "blocks": current_index
                })

            return {"index": json.dumps(global_index, indent=2),
                    "deep_index": json.dumps(deep_index, indent=2),
                    "sandwiches": sandwiches}
        except Exception as e:
            logging.error(f"#ERROR: Failed to pack blocks: {str(e)}")
            traceback.print_exc()
            raise


</python>
<python src="/lib/vue_block.py" mod_time="2025-07-15 13:00:22Z">
# /lib/vue_block.py, updated 2025-07-15 15:43 EEST
import re
import logging
from typing import Dict
from lib.content_block import ContentBlock
from lib.sandwich_pack import SandwichPack

class ContentCodeVue(ContentBlock):
    supported_types = [".vue"]

    def __init__(self, content_text: str, content_type: str, file_name: str, timestamp: str, **kwargs):
        super().__init__(content_text, content_type, file_name, timestamp, **kwargs)
        self.tag = "vue"
        logging.debug(f"Initialized ContentCodeVue with tag={self.tag}, file_name={file_name}")

    def parse_content(self) -> Dict:
        entities = []
        dependencies = {"modules": [], "imports": [], "calls": []}
        component_pattern = re.compile(r"defineComponent\s*\(\s*{", re.DOTALL | re.MULTILINE)
        for match in component_pattern.finditer(self.content_text):
            full_text = self._extract_full_entity(match.start(), match.end())
            entities.append({"type": "component", "name": "VueComponent", "visibility": "public", "tokens": self._estimate_tokens(full_text)})
        import_pattern = re.compile(r"import\s+{?([\w,\s]+)}?\s+from\s+['\"]([^'\"]+)['\"]", re.MULTILINE)
        for match in import_pattern.finditer(self.content_text):
            items = [item.strip() for item in match.group(1).split(",")]
            for item in items:
                dependencies["imports"].append(item)
        return {"entities": entities, "dependencies": {k: sorted(list(set(v))) for k, v in dependencies.items()}}

    def _extract_full_entity(self, start: int, end_header: int) -> str:
        brace_count = 1
        i = end_header
        while i < len(self.content_text) and brace_count > 0:
            if self.content_text[i] == '{':
                brace_count += 1
            elif self.content_text[i] == '}':
                brace_count -= 1
            i += 1
        return self.content_text[start:i] if brace_count == 0 else self.content_text[start:end_header]

    def _estimate_tokens(self, content: str) -> int:
        return len(content) // 4

SandwichPack.register_block_class(ContentCodeVue)
</python>
<python src="/lib/__init__.py" mod_time="2025-07-15 06:32:56Z">
# /lib/__init__.py, created 2025-07-15 09:35 EEST


</python>
