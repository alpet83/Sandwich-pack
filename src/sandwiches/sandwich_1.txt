<python file_id="0" mod_time="2025-08-07 08:36:29Z" relevance="0">
# /spack.py, updated 2025-08-07 11:36 EEST
import os
import datetime
import logging
import argparse
from pathlib import Path
from lib.sandwich_pack import SandwichPack

logging.basicConfig(level=logging.DEBUG, format='%(asctime)s #%(levelname)s: %(message)s')

def get_file_mod_time(file_path):
    mtime = os.path.getmtime(file_path)
    mod_time = datetime.datetime.fromtimestamp(mtime, datetime.UTC)
    return mod_time.strftime("%Y-%m-%d %H:%M:%SZ")

def is_hidden_file(filepath):
    return any(part.startswith(".") for part in filepath.parts)

def collect_files(root_dir):
    content = []
    root_path = Path(root_dir).parent
    logging.debug(f"Scanning directory: {root_dir}")
    if not os.path.exists(root_dir):
        logging.error(f"Directory {root_dir} does not exist")
        return content
    for file_path in Path(root_dir).rglob("*"):
        if file_path.is_file() and not is_hidden_file(file_path):
            relative_path = f"/{file_path.relative_to(root_path)}".replace("\\", "/")
            extension = Path(file_path).suffix.lower()
            content_type = extension if extension else ""
            if not content_type or not SandwichPack.supported_type(content_type):
                logging.debug(f"Skipping unsupported content_type: {content_type} for {relative_path}")
                continue
            try:
                with open(file_path, "r", encoding="utf-8-sig", errors="replace") as f:
                    text = f.read()
                logging.debug(f"Read {file_path} with encoding: utf-8-sig")
            except UnicodeDecodeError as e:
                logging.warning(f"Non-UTF-8 characters in {file_path}, replaced with �: {e}")
                continue
            mod_time = get_file_mod_time(file_path)
            logging.debug(f"Collected file: {relative_path} with content_type: {content_type}")
            content.append(SandwichPack.create_block(
                content_text=text,
                content_type=content_type,
                file_name=relative_path,
                timestamp=mod_time
            ))
    for file_path in root_path.glob("*.toml"):
        if not is_hidden_file(file_path):
            relative_path = f"/{file_path.name}".replace("\\", "/")
            content_type = ".toml"
            if not SandwichPack.supported_type(content_type):
                logging.debug(f"Skipping unsupported content_type: {content_type} for {relative_path}")
                continue
            try:
                with open(file_path, "r", encoding="utf-8-sig", errors="replace") as f:
                    text = f.read()
                logging.debug(f"Read {file_path} with encoding: utf-8-sig")
            except UnicodeDecodeError as e:
                logging.warning(f"Non-UTF-8 characters in {file_path}, replaced with �: {e}")
                continue
            mod_time = get_file_mod_time(file_path)
            logging.debug(f"Collected file: {relative_path} with content_type: {content_type}")
            content.append(SandwichPack.create_block(
                content_text=text,
                content_type=content_type,
                file_name=relative_path,
                timestamp=mod_time
            ))
    return content

def main():
    logging.info("Starting spack CLI")
    SandwichPack.load_block_classes()
    project_dir = "."
    output_dir = "./sandwiches"
    files_content = collect_files(project_dir)
    if not files_content:
        logging.error("No files collected, exiting")
        raise SystemExit("Error: No files found in the specified directory")
    logging.info(f"Collected {len(files_content)} files")
    parser = argparse.ArgumentParser(
        prog='Sandwich Packer',
        description='Combining all project sources files into sandwich structured several text files',
        epilog='Best for using with chatbots like Grok or ChatGPT, with expert level')

    parser.add_argument('project_name')
    args = parser.parse_args()
    packer = SandwichPack(args.project_name)
    result = packer.pack(files_content)
    os.makedirs(output_dir, exist_ok=True)
    for i, sandwich in enumerate(result["sandwiches"], 1):
        output_file = Path(output_dir) / f"sandwich_{i}.txt"
        with open(output_file, "w", encoding="utf-8") as f:
            f.write(sandwich + "\nINDEX_COPY:\n" + result["index"])
        logging.info(f"Created {output_file} ({len(sandwich.encode('utf-8'))} bytes)")
    global_index_file = Path(output_dir) / "sandwiches_index.jsl"
    with open(global_index_file, "w", encoding="utf-8") as f:
        f.write(result["index"] + "STRUCTURE: " + result["deep_index"])

    global_index_file = Path(output_dir) / "sandwiches_structure.json"
    with open(global_index_file, "w", encoding="utf-8") as f:
        f.write(result["deep_index"])

    logging.info(f"Created {global_index_file}")

if __name__ == "__main__":
    main()

</python>
<python file_id="1" mod_time="2025-08-06 15:48:54Z" relevance="0">
# /spack_verify.py, created 2025-08-06 18:43 EEST
# Verifies presence of opening tags for each file_id in sandwich files, checking start_line from sandwiches_structure.json

import json
import logging
import re
from pathlib import Path

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s #%(levelname)s: %(message)s'
)

def verify_sandwiches(index_file, structure_file, sandwich_dir="."):
    """Verify opening tags for each file_id in sandwiches, matching start_line from structure."""
    try:
        sandwich_path = Path(sandwich_dir)
        # Load sandwiches_index.json
        with open(sandwich_path / index_file, "r", encoding="utf-8") as f:
            index = json.load(f)
        file_map = {int(line.split(",")[0]): line.split(",")[1] for line in index["files"]}
        logging.info(f"Loaded {len(file_map)} files from {index_file}")

        # Load sandwiches_structure.json
        with open(sandwich_path/ structure_file, "r", encoding="utf-8") as f:
            structure = json.load(f)
        sandwiches = structure["sandwiches"]
        logging.info(f"Loaded {len(sandwiches)} sandwich entries from {structure_file}")

        # Track found and missing file_ids
        found_files = set()
        missing_files = set(file_map.keys())

        # Process each sandwich file
        for sandwich in sandwiches:
            sandwich_file = sandwich_path / sandwich["file"]
            if not sandwich_file.exists():
                logging.error(f"Sandwich file {sandwich_file} not found in {sandwich_dir}")
                continue

            with open(sandwich_file, "r", encoding="utf-8") as f:
                lines = f.readlines()

            # Check each block in the sandwich
            for block in sandwich["blocks"]:
                file_id = block["file_id"]
                start_line = block["start_line"]
                file_name = file_map.get(file_id, f"Unknown (file_id={file_id})")

                # Verify start_line is within bounds
                if start_line < 1 or start_line > len(lines):
                    logging.warning(f"Start line {start_line} out of range for file_id={file_id} ({file_name}) in {sandwich_file}")
                    continue

                # Check for opening tag at start_line
                line = lines[start_line - 1].strip()
                tag_pattern = r'<\s*(python|php|jss|rustc|tss|vue)\s+[^>]*file_id="' + str(file_id) + r'"[^>]*>'
                if re.match(tag_pattern, line):
                    logging.info(f"Opening tag found for file_id={file_id} ({file_name}) at start_line={start_line} in {sandwich_file}: {line}")
                    found_files.add(file_id)
                else:
                    logging.warning(f"Opening tag not found for file_id={file_id} ({file_name}) at start_line={start_line} in {sandwich_file}. Line: {line}")

        # Report missing files
        missing_files -= found_files
        if missing_files:
            logging.error(f"Missing files: {', '.join(f'file_id={fid} ({file_map.get(fid, "Unknown")})' for fid in sorted(missing_files))}")
        else:
            logging.info("All files from sandwiches_index.json found in sandwiches")

        return len(missing_files) == 0

    except Exception as e:
        logging.error(f"Verification failed: {str(e)}")
        return False

if __name__ == "__main__":
    index_file = "sandwiches_index.json"
    structure_file = "sandwiches_structure.json"
    sandwich_dir = "./sandwiches"  # Adjust if sandwiches are in a different directory
    success = verify_sandwiches(index_file, structure_file, sandwich_dir)
    if not success:
        logging.error("Verification failed: Some files are missing or have incorrect start lines")
</python>
<python file_id="2" mod_time="2025-08-06 10:30:03Z" relevance="0">
# /lib/code_stripper.py, updated 2025-08-06 13:00 EEST
# Formatted with proper line breaks and indentation for project compliance.
# Proposed: 2025-08-06
# Changes: Added start_offset to detect_single, updated strip to process single-line content cyclically until no matches, skip empty lines to avoid redundant processing (CLA Rule 2: Ensure code correctness, CLA Rule 12: Minimize changes).

import logging
import re
from abc import ABC, abstractmethod

class CodeStripper(ABC):
    """Base class for stripping content (strings or comments) from code lines."""
    def __init__(self, owner):
        self.owner = owner
        self.strip_log = []
        self.warnings = []
        self.sl_open = []
        self.ml_open = []
        self.ml_close = []
        logging.debug(f"Initialized {self.__class__.__name__} for file {owner.file_name}")

    @abstractmethod
    def detect_single(self, line: str, line_num: int, start_offset: int) -> tuple:
        """Detects single-line content to strip, returning (start_pos, end_pos)."""
        pass

    @abstractmethod
    def detect_multi_open(self, line: str) -> tuple:
        """Detects multi-line content opening, returning (start_pos, token_index)."""
        pass

    @abstractmethod
    def detect_multi_close(self, line: str, close_token: str, start_offset: int) -> tuple:
        """Detects multi-line content closing, returning (start_pos, token_length)."""
        pass

    def strip(self, lines: list) -> list:
        """Strips content from lines, preserving empty lines."""
        if len(lines) <= 1:
            raise Exception("Lines not initialized")
        result_lines = lines.copy()
        in_multi = False
        close_token = None
        open_line = -1
        total_ml = 0
        multi_start_pos = -1
        log_indent = "\tSTRIP:"

        for line_num, line in enumerate(lines[1:], 1):
            if not isinstance(line, str):
                result_lines[line_num] = f"// NOT AS STRING, LINE {line_num}"
                continue
            if not line.strip():
                result_lines[line_num] = line
                continue
            clean_line = line
            end_pos = 0
            while end_pos < len(clean_line):
                start_pos, end_pos = self.detect_single(clean_line, line_num, end_pos)
                if start_pos < 0:
                    break
                clean_line = clean_line[:start_pos] + clean_line[end_pos:]
                clean_line = clean_line.rstrip()   # после очистки однострочных комментариев, пустые строки должны стать нулевой длины
                self.strip_log.append(f"{log_indent} Single-line content at line {line_num}, pos {start_pos}-{end_pos}, stripped: '{clean_line}', line: '{line}'")
                result_lines[line_num] = clean_line
                end_pos += 1

            left_part = ""
            if not in_multi and clean_line.strip():
                start_pos, token_index = self.detect_multi_open(clean_line)
                if start_pos >= 0:
                    left_part = clean_line[:start_pos]    # здесь нельзя модифицировать clean_line, поскольку запоминается multi_start_pos
                    in_multi = True
                    open_line = line_num
                    multi_start_pos = start_pos
                    close_token = self.ml_close[token_index]
                    self.strip_log.append(f"{log_indent} Multi-line content started at line {line_num}, pos {start_pos}, line: '{line}'")

            if in_multi and close_token:
                same_line = line_num == open_line
                total_ml += 1
                end_pos, token_length = self.detect_multi_close(clean_line, close_token, multi_start_pos if same_line else 0)
                if end_pos >= 0:
                    rest = left_part + clean_line[end_pos + token_length:]
                    result_lines[line_num] = rest
                    in_multi = False
                    close_token = None
                    self.strip_log.append(f"{log_indent} Multi-line content ended at line {line_num}, pos {end_pos}, remaining: '{rest}', line: '{line}'")
                else:
                    if line_num > open_line:
                        self.strip_log.append(f"{log_indent} Multi-line content continued at line {line_num}, line: '{line}'")
                    result_lines[line_num] = left_part
                    continue

            if in_multi and line_num == len(lines) - 1:
                msg = f"{log_indent} Incomplete multi-line content in file {self.owner.file_name} at line {line_num}"
                self.owner.parse_warn(msg)
                self.strip_log.append(msg)
                for j in range(line_num + 1, len(lines)):
                    result_lines[j] = clean_line
                    self.strip_log.append(f"{log_indent} Multi-line unclosed content continued at line {j}, line: '{lines[j]}'")
            result_lines[line_num] = clean_line

        logging.debug(f"{log_indent}Total multi-line content lines: {total_ml} / {len(result_lines)}")
        return result_lines

class CodeStringStripper(CodeStripper):
    """Strips string literals (single-line, raw, and multi-line)."""
    def __init__(self, owner, string_quote_chars, raw_str_prefix, raw_quote_char, open_ml_string, close_ml_string, escape_char):
        super().__init__(owner)
        self.sl_open = list(string_quote_chars)
        self.ml_open = open_ml_string
        self.ml_close = close_ml_string
        self.raw_str_prefix = raw_str_prefix
        self.raw_quote_char = raw_quote_char
        self.escape_char = escape_char

    def detect_single(self, line: str, line_num: int, start_offset: int) -> tuple:
        """Detects single-line string literals, returning positions of content between quotes."""
        in_string = False
        quote_char = None
        start_pos = -1
        end_pos = -1
        _rsq_len = len(self.raw_str_prefix) if self.raw_str_prefix else 0
        i = start_offset
        while i < len(line):
            char = line[i]
            if not in_string:
                prefix_chars = line[i - _rsq_len:i] if i >= _rsq_len else None
                _is_raw_start = self.raw_str_prefix and prefix_chars == self.raw_str_prefix
                _is_quote_char = char in self.sl_open or (self.raw_quote_char and char == self.raw_quote_char)
                if _is_quote_char or (_is_raw_start and _is_quote_char):
                    in_string = True
                    quote_char = char
                    start_pos = i + (_rsq_len if _is_raw_start else 1)
                    i += _rsq_len if _is_raw_start else 1
                    continue
            elif in_string and char == self.escape_char and i + 1 < len(line):
                i += 2
                continue
            elif in_string and char == quote_char:
                in_string = False
                end_pos = i
                quote_char = None
                i += 1
                return start_pos, end_pos
            i += 1
        if in_string:
            self.owner.parse_warn(f"Incomplete string literal in file {self.owner.file_name} at line {line_num}")
            self.strip_log.append(f"Incomplete string at line {line_num}, line: '{line}'")
            return start_pos, len(line)
        return -1, -1

    def detect_multi_open(self, line: str) -> tuple:
        """Detects multi-line string opening."""
        for j, open_quote in enumerate(self.ml_open):
            match = re.search(re.escape(open_quote), line)
            if match:
                return match.start(), j
        return -1, -1

    def detect_multi_close(self, line: str, close_token: str, start_offset: int) -> tuple:
        """Detects multi-line string closing, starting from start_offset."""
        match = re.search(re.escape(close_token), line[start_offset:])
        if match:
            return start_offset + match.start(), len(close_token)
        return -1, -1

class CodeCommentStripper(CodeStripper):
    """Strips comments (single-line and multi-line)."""
    def __init__(self, owner, open_sl_comment: list, open_ml_comment: list, close_ml_comment: list):
        super().__init__(owner)
        self.sl_open = open_sl_comment
        self.ml_open = open_ml_comment
        self.ml_close = close_ml_comment

    def detect_single(self, line: str, line_num: int, start_offset: int) -> tuple:
        """Detects single-line comments."""
        first_comm_pos = -1
        for sl_comment in self.sl_open:
            start_pos = line.find(sl_comment, start_offset)
            if start_pos >= 0 and (first_comm_pos < 0 or start_pos < first_comm_pos):
                first_comm_pos = start_pos
        return first_comm_pos, len(line)

    def detect_multi_open(self, line: str) -> tuple:
        """Detects multi-line comment opening."""
        for j, ml_comment in enumerate(self.ml_open):
            match = re.search(ml_comment, line, re.IGNORECASE)
            if match:
                return match.start(), j
        return -1, -1

    def detect_multi_close(self, line: str, close_token: str, start_offset: int) -> tuple:
        """Detects multi-line comment closing, starting from start_offset."""
        match = re.search(close_token, line[start_offset:], re.IGNORECASE)
        if match:
            return start_offset + match.start(), len(close_token)
        return -1, -1
</python>
<python file_id="3" mod_time="2025-08-08 15:21:37Z" relevance="0">
# /lib/content_block.py, updated 2025-08-06 14:30 EEST
# Formatted with proper line breaks and indentation for project compliance.
# Proposed: 2025-08-06
# Changes: Added module_map parameter to compress for imported class name compression (e.g., SandwichPack), added log separator (CLA Rule 2: Ensure code correctness, CLA Rule 12: Minimize changes).

import logging
import re
import os
import math
from pathlib import Path
from .deps_builder import DepsParser
from .llm_tools import estimate_tokens
from .code_stripper import CodeStringStripper, CodeCommentStripper

# PROTECTION CODE DON'T TOUCH, typing is disabled!!!
Optional = List = Tuple = Dict = None

logging.basicConfig(
    level=os.environ.get('LOGLEVEL', 'DEBUG').upper()
)

class ContentBlock:
    supported_types = [':document', ':post']

    def __init__(self, content_text, content_type, file_name=None, timestamp=None, **kwargs):
        self.content_text = content_text
        self.content_type = content_type
        self.tag = "post" if content_type == ":post" else "document"
        self.parsers = []
        self.dependencies = {"modules": [], "imports": {}}
        self.file_name = file_name
        self.timestamp = timestamp
        self.call_method_sep = ['.']
        self.post_id = kwargs.get('post_id')
        self.user_id = kwargs.get('user_id')
        self.relevance = kwargs.get('relevance', 0)
        self.file_id = kwargs.get('file_id')
        self.tokens = estimate_tokens(content_text)
        self.clean_lines = ["Line №0"] + self.content_text.splitlines()
        self.strip_log = []
        self.warnings = []
        self.entity_map = {}
        self.string_quote_chars = "\"'"
        self.raw_str_prefix = None
        self.raw_quote_char = None
        self.open_ml_string = []
        self.close_ml_string = []
        self.open_sl_comment = ["//"]
        self.open_ml_comment = [r"/\*"]
        self.close_ml_comment = [r"\*/"]
        self.escape_char = "\\"
        self.module_prefix = ""
        self.line_offsets = []
        logging.debug(f"Initialized ContentBlock with content_type={content_type}, tag={self.tag}, file_name={file_name}")

    def parse_warn(self, msg):
        """Logs a warning and adds it to self.warnings."""
        self.warnings.append(msg)
        logging.warning(msg)

    def strip_strings(self):
        """Strips string literals using CodeStringStripper."""
        if len(self.clean_lines) <= 1:
            self.clean_lines = [''] + self.content_text.splitlines()
        stripper = CodeStringStripper(
            self,
            string_quote_chars=self.string_quote_chars,
            raw_str_prefix=self.raw_str_prefix,
            raw_quote_char=self.raw_quote_char,
            open_ml_string=self.open_ml_string,
            close_ml_string=self.close_ml_string,
            escape_char=self.escape_char
        )
        self.clean_lines = stripper.strip(self.clean_lines)
        self.strip_log.extend(stripper.strip_log)
        self.warnings.extend(stripper.warnings)
        self.get_clean_content()
        return self.clean_lines

    def strip_comments(self):
        """Strips comments using CodeCommentStripper."""
        if len(self.clean_lines) <= 1:
            raise Exception("clean_lines not filled")
        stripper = CodeCommentStripper(
            self,
            open_sl_comment=self.open_sl_comment,
            open_ml_comment=self.open_ml_comment,
            close_ml_comment=self.close_ml_comment
        )
        self.clean_lines = stripper.strip(self.clean_lines)
        self.strip_log.extend(stripper.strip_log)
        self.warnings.extend(stripper.warnings)
        self.get_clean_content()
        return self.clean_lines

    def save_clean(self, file_name):
        """Saves the cleaned content to a file for debugging, replacing empty lines with line number comments."""
        if len(self.clean_lines) <= 1:
            raise Exception("clean_lines not filled")
        try:
            output_path = Path(file_name)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            with output_path.open("w", encoding="utf-8") as f:
                for line_num, line in enumerate(self.clean_lines[1:], 1):
                    f.write((line if line.strip() else f"// Line {line_num}") + "\n")
            logging.debug(f"Saved cleaned content to {file_name}")
        except Exception as e:
            logging.error(f"Failed to save cleaned content to {file_name}: {str(e)}")

    def get_clean_content(self):
        """Returns the cleaned content as a single string and updates line_offsets."""
        if len(self.clean_lines) <= 1:
            raise Exception("clean_lines not initialized")
        content = "\n".join(self.clean_lines[1:])
        self.line_offsets = [0]
        offset = 0
        for line in self.clean_lines[1:]:
            offset += len(line) + 1
            self.line_offsets.append(offset)
        # logging.debug(f"Updated line_offsets: {self.line_offsets[:10]}... (total {len(self.line_offsets)})")
        return content

    def find_line(self, content_offset):
        """Finds the line number for a given content offset."""
        if not self.line_offsets:
            self.get_clean_content()
        for i, offset in enumerate(self.line_offsets):
            if content_offset < offset:
                return i
        return len(self.line_offsets) - 1

    def count_chars(self, line_num, ch):
        """Counts occurrences of a character in a specific line of clean code."""
        if len(self.clean_lines) <= 1:
            raise Exception("clean_lines not initialized")
        if line_num < 1 or line_num >= len(self.clean_lines):
            logging.error(f"Invalid line number {line_num} for file {self.file_name}")
            return 0
        line = self.clean_lines[line_num]
        if not isinstance(line, str):
            return 0
        return line.count(ch)

    def sorted_entities(self):
        """Sorts entities by their line number."""
        sorted_map = {}
        result = []
        for line_num in sorted(self.entity_map.keys()):
            sorted_map[line_num] = self.entity_map[line_num]
            result.append(self.entity_map[line_num])
        self.entity_map = sorted_map
        return result

    def detect_bounds(self, start_line, clean_lines):
        """Detects the start and end line of an entity using brace counting."""
        if start_line < 1 or start_line >= len(clean_lines) or not clean_lines[start_line] or not clean_lines[start_line].strip():
            logging.error(f"Invalid start line {start_line} for file {self.file_name} module [{self.module_prefix}]")
            return start_line, start_line
        brace_count = 0
        line_num = start_line
        # поиск открывающей скобки, для варианта когда start_line указывает на начало многострочного определения функции/метода
        for i in range(8):
            line = clean_lines[line_num]
            if line.count('{') > 0:
                break
            line_num += 1

        while line_num < len(clean_lines):
            line = clean_lines[line_num]
            if not isinstance(line, str) or not line.strip():
                line_num += 1
                continue
            brace_count += line.count('{') - line.count('}')
            if brace_count == 0 and line_num >= start_line:
                return start_line, line_num
            line_num += 1
        self.parse_warn(f"Incomplete entity at line {start_line} in file {self.file_name}, brace_count={brace_count}")
        return start_line, start_line

    def check_entity_placement(self, line_num: int, name: str):
        """Checks if an entity with the given name is correctly placed at line_num."""
        if line_num < 1 or line_num >= len(self.clean_lines) or not self.clean_lines[line_num]:
            logging.warning(f" check_entity_placement failed: outbound {line_num} or void line")
            return False
        line = self.clean_lines[line_num]
        base_name = name.split(".")[-1]
        base_name = base_name.split("::")[-1].split("<")[0]
        pattern = rf"\b{base_name}\b"
        result = bool(re.search(pattern, line))
        if not result:
            best = 10
            for i, search_line in enumerate(self.clean_lines[1:], 1):
                if isinstance(search_line, str) and re.search(pattern, search_line):
                    diff = line_num - i
                    best = min(best, abs(diff))
                    logging.debug(f"  occurrence of '{base_name}' found at line {i}: '{search_line}'")
            result = abs(best) <= 1

        # logging.debug(f"Checking entity placement for {name} at line {line_num}: {'Passed' if result else 'Failed'}, line: '{line}'")
        return result

    def add_entity(self, line_num: int, entity: dict):
        """Adds an entity to entity_map with placement and duplication checks."""
        if line_num in self.entity_map:
            existing = self.entity_map[line_num]
            if existing["name"] != entity["name"] or existing["type"] != entity["type"]:
                logging.warning(f"Duplicate entity at line {line_num}: {entity['name']} ({entity['type']}) conflicts with {existing['name']} ({existing['type']})")
                return False
        if not self.check_entity_placement(line_num, entity["name"]):
            logging.warning(f"Entity {entity['name']} placement check failed at line {line_num}, line: '{self.clean_lines[line_num]}'")
            return False
        entity["first_line"] = line_num
        if entity["type"] != "abstract method" or "last_line" not in entity:
            entity["last_line"] = self.detect_bounds(line_num, self.clean_lines)[1]
        self.entity_map[line_num] = entity
        logging.debug(f"Added entity {entity['name']} at first_line={line_num}, last_line={entity['last_line']}")
        return True

    def extract_entity_text(self, def_start: int, def_end: int) -> str:
        """Extracts the full entity text using clean_lines for brace counting."""
        content = self.get_clean_content()
        start_line = self.find_line(def_end)  # открывающая реальная скобка должна находиться тут
        start_line, end_line = self.detect_bounds(start_line, self.clean_lines)
        if start_line == end_line:
            self.parse_warn(f"Incomplete/abstract entity in file {self.file_name} at start={start}, line @{start_line} using header end")
            return content[start:].splitlines()[0]
        logging.info(f"Extracted entity from first_line={start_line} to last_line={end_line}")
        return "\n".join(self.clean_lines[start_line:end_line + 1])

    def extend_deps(self, parser):
        m = []
        i = {}
        deps = parser
        if getattr(parser, 'dependencies', False):
            deps = parser.dependencies
        i = getattr(deps, 'imports', {})
        m = getattr(deps, 'modules', [])
        unique = set(self.dependencies['modules'])
        unique.update(m)
        self.dependencies['modules'] = list(unique)
        self.dependencies['imports'].update(i)

    def full_text_replace(self, from_str: str, entity_id: int, ent_type: str, is_definition: bool = False) -> str:
        """Performs context-aware replacement of entity names with \x0F<entity_id>.

        Args:
            from_str (str): The entity name to replace.
            entity_id (int): The global entity index to replace with.
            ent_type (str): The type of entity (function, local_function, method, abstract method, structure, class, interface, module, component, object).
            is_definition (bool): If True, replace in definition line without context restrictions.

        Returns:
            str: The content with replaced entity names.
        """
        if is_definition:
            pattern = rf"\b{re.escape(from_str)}\b"
        else:
            if ent_type in ("function", "local_function"):
                pattern = rf"(?:(?<=[\s])|\b){re.escape(from_str)}(?=\s|\()"
            elif ent_type in ("method", "abstract method"):
                pattern = rf"(?<=[\s.->]){re.escape(from_str)}(?=\s|\()"
            elif ent_type == "structure":
                pattern = rf"(?<=[\s.->|<]){re.escape(from_str)}(?=\s|\(|<|>)"
            elif ent_type in ("class", "interface"):
                pattern = rf"(?<=[\s|=\(]){re.escape(from_str)}(?=\s|\(|\.|,|;|\))*"  # варианты использования классов: конструкция, наследование, вызов статического метода, импорт в заголовке
            else:
                pattern = rf"\b{re.escape(from_str)}\b"
        compressed = re.sub(pattern, f"\x0F{entity_id}", self.content_text)
        if compressed == self.content_text:
            logging.warning(f"Failed replace '{from_str}' with '\x0F{entity_id}' in {self.file_name} (type={ent_type}, is_definition={is_definition})")
        else:
            logging.debug(f"Replaced '{from_str}' with '\x0F{entity_id}' in {self.file_name} (type={ent_type}, is_definition={is_definition})")
            self.content_text = compressed

    def compress(self, entity_rev_map, file_map: dict):
        """Compresses entity names in content_text to their global indexes prefixed with ANSI \x0F.

        Args:
            entity_rev_map: Dictionary mapping (file_id, ent_type, ent_name) to entity_id.
            file_map: Dictionary file names to file_id.
        """
        if self.content_type == ":post":
            logging.debug(f"No compression for post content: {self.file_name}")
            return
        file_index = {}
        for file_name, file_id in file_map.items():
            file_index[file_id] = file_name
        ent_index = {}
        for (file_id, ent_type, ent_name) in entity_rev_map:
            ent_index[ent_name] = (file_id, ent_type, ent_name)

        logging.debug(f"------- compressing {self.file_name} -------")
        original_length = len(self.content_text)
        valid_entities = {}
        name_count = {}
        for entity in self.entity_map.values():
            ent_name = entity["name"]
            name_count[ent_name] = name_count.get(ent_name, 0) + 1

        for line_num, entity in self.entity_map.items():
            ent_type = entity["type"]
            ent_name = entity["name"]
            if name_count[ent_name] > 1:
                logging.debug(f"SKIP_ENTITY: non unique name {ent_name}")
                continue
            if ent_type in ("function", "local_function", "class", "interface", "structure", "method", "abstract method", "module", "component", "object"):
                valid_entities[ent_name] = (self.file_id, ent_type, line_num)
                logging.debug(f"Added local entity {ent_name} ({ent_type}, file_id={self.file_id}, line={line_num}) for compression")
            if "parent" in entity and entity["parent"]:
                parent_name = entity["parent"]
                for ent_type in ("class", "interface"):
                    key = (self.file_id, ent_type, parent_name)
                    if key in entity_rev_map:
                        valid_entities[parent_name] = (self.file_id, ent_type, None)
                        logging.debug(f"Added parent entity {parent_name} ({ent_type}, file_id={self.file_id}) for compression")

        for parser in getattr(self, "parsers", []):
            if isinstance(parser, DepsParser):
                checked = 0
                for ent_name in parser.imports.keys():
                    checked += 1
                    (file_id, ent_type, ent_exists) = ent_index.get(ent_name, (-1, None, None))
                    if file_id >= 0:
                        file_name = file_index.get(file_id, "unknown")
                        if ent_name == ent_exists:
                            # TODO: можно добавить проверку для коротких имен, на соответствие mod_name и file_name
                            mod_name = parser.imports[ent_name]
                            valid_entities[ent_name] = (file_id, ent_type, None)
                            logging.debug(f"Added imported entity {ent_name} ({ent_type}, mod={mod_name}, file_id={file_id}), file_name=`{file_name}` for compression")
                    else:
                        logging.warning(f"Failed locate imported entity {ent_name}")
                logging.debug(f"Checked {checked} from imports: {parser.imports}")

        compressed_count = 0
        for ent_name, (file_id, ent_type, line_num) in valid_entities.items():
            key = (file_id, ent_type, ent_name)
            is_definition = False
            if line_num is not None and line_num in self.entity_map:
                line = self.clean_lines[line_num]
                # TODO: тут надо заменить проверку на простое соответствие линии определения сущности
                if isinstance(line, str) and re.search(rf"\b(def|function|class|struct|impl|mod)\s+{re.escape(ent_name)}\b", line):
                    is_definition = True
            if file_id is None:
                for fid in {f[0] for f in entity_rev_map.keys()}:
                    test_key = (fid, ent_type, ent_name)
                    if test_key in entity_rev_map:
                        index = entity_rev_map[test_key]
                        self.full_text_replace(ent_name, index, ent_type, is_definition)
                        compressed_count += 1
                        break
            elif key in entity_rev_map:
                index = entity_rev_map[key]
                self.full_text_replace(ent_name, index, ent_type, is_definition)
                compressed_count += 1

        self.tokens = estimate_tokens(self.content_text)
        compressed_length = len(self.content_text)
        logging.info(f"Compressed {compressed_count} entities in {self.file_name}, "
                     f"original length: {original_length}, compressed length: {compressed_length}, "
                     f"reduced by {original_length - compressed_length} characters, new tokens: {self.tokens}")

    def to_sandwich_block(self):
        """Convert block to sandwich format with attributes mapped via dictionary."""
        # Dictionary mapping tag attributes to object field names
        attr_to_field = {
            'post_id': 'post_id'
        } if self.content_type == ':post' else {
            'file_id': 'file_id',
            'mod_time': 'timestamp'
        }
        attr_to_field['user_id'] = 'user_id'
        attr_to_field['relevance'] = 'relevance'

        # Build tag attributes, excluding None or irrelevant fields
        attrs = []
        for attr, field in attr_to_field.items():
            value = getattr(self, field, None)
            if value is not None:
                attrs.append(f'{attr}="{value}"')
        attr_str = " ".join(attrs)
        return f"<{self.tag} {attr_str}>\n{self.content_text}\n</{self.tag}>"

    def parse_content(self, clean_lines=None, depth=0):
        return {"entities": [], "dependencies": self.dependencies}

</python>
<python file_id="4" mod_time="2025-08-08 13:25:05Z" relevance="0">
# /lib/deps_builder.py, created 2025-08-05 11:00 EEST
# Formatted with proper line breaks and indentation for project compliance.

import logging
import os
from .entity_parser import EntityParser

logging.basicConfig(level=logging.DEBUG, format='%(asctime)s #%(levelname)s: %(message)s')


class DepsParser(EntityParser):
    """Base class for parsing dependencies in content blocks."""
    def __init__(self, owner, outer_regex):
        self.owner = owner
        self.import_pattern = outer_regex
        super().__init__("dependence", owner, outer_regex, r"", default_visibility="public")

    def add_module(self, name):
        """Add a module to the dependencies list."""
        if name and name not in self.modules:
            self.modules.append(name)
            logging.debug(f"Added module import: {name} for file {self.owner.file_name}")

    def add_import(self, mod_name, ent_name):
        """Add an import of an entity from a module."""
        if mod_name and ent_name:
            self.imports[ent_name] = mod_name
            logging.debug(f"Added entity import: {ent_name} from {mod_name} for file {self.owner.file_name}")

    def store_deps(self, dest: dict) -> dict:
        dest['imports'].extend(self.imports)
        unique = set(dest['modules'])
        unique.update(self.modules)
        dest['modules'] = list(unique)
        dest[d].sort()
        return dest


def organize_modules(file_list, blocks):
    """Sort blocks and file_list to ensure modules appear before their dependents."""
    if len(file_list) != len(blocks):
        logging.error(f"Mismatch between file_list ({len(file_list)}) and blocks ({len(blocks)})")
        return file_list, blocks

    file_map = {}
    for i, file_entry in enumerate(file_list):
        file_id, file_name = file_entry.split(',', 1)[:2]
        file_map[file_name] = i

    dep_graph = {i: set() for i in range(len(blocks))}
    for i, block in enumerate(blocks):
        for parser in getattr(block, "parsers", []):
            if isinstance(parser, DepsParser):
                for ent_name, mod_name in parser.imports.items():
                    mod_path = f"/{mod_name.replace('.', '/')}.py" if mod_name.startswith('lib.') else f"/tests/{mod_name}.py"
                    if mod_path in file_map:
                        dep_graph[i].add(file_map[mod_path])
                for mod_name in parser.modules:
                    mod_path = f"/{mod_name.replace('.', '/')}.py" if mod_name.startswith('lib.') else f"/tests/{mod_name}.py"
                    if mod_path in file_map:
                        dep_graph[i].add(file_map[mod_path])

    sorted_indices = []
    visited = set()
    temp_mark = set()

    def dfs(index):
        if index in temp_mark:
            logging.warning(f"Circular dependency detected at index {index}")
            return
        if index not in visited:
            temp_mark.add(index)
            for dep in dep_graph[index]:
                dfs(dep)
            temp_mark.remove(index)
            visited.add(index)
            sorted_indices.append(index)

    for i in range(len(blocks)):
        if i not in visited:
            dfs(i)

    sorted_file_list = [file_list[i] for i in sorted_indices]
    sorted_blocks = [blocks[i] for i in sorted_indices]

    new_file_ids = {}
    for i, file_entry in enumerate(sorted_file_list):
        old_file_id, rest = file_entry.split(',', 1)
        new_file_ids[old_file_id] = i
        sorted_file_list[i] = f"{i},{rest}"

    for block in sorted_blocks:
        if block.file_id is not None:
            block.file_id = new_file_ids.get(str(block.file_id), block.file_id)
        for entity in block.entity_map.values():
            if 'file_id' in entity:
                entity['file_id'] = new_file_ids.get(str(entity['file_id']), entity['file_id'])

    logging.debug(f"Sorted file_list: {sorted_file_list}")
    return sorted_file_list, sorted_blocks
</python>

INDEX_COPY:
{
  "packer_version": "0.6",
  "context_date": "2025-08-09 16:40:12Z",
  "templates": {
    "filelist": "file_id,file_name,md5,tokens,timestamp",
    "users": "user_id,username,role",
    "entities": "vis(pub/prv),type,parent,name,file_id,start_line-end_line,tokens"
  },
  "project_name": "sandwich_pack",
  "datasheet": {
    "project_root": "/app",
    "backend_address": "http://localhost:8080",
    "frontend_address": "http://vps.vpn:8008",
    "resources": {
      "database": "sqlite:///app/data/multichat.db",
      "log_file": "/app/logs/colloquium_core.log"
    }
  },
  "entities": [
    "pub,function,,get_file_mod_time,0,11-14,54",
    "pub,function,,is_hidden_file,0,16-17,30",
    "pub,function,,collect_files,0,19-71,493",
    "pub,function,,main,0,73-106,333",
    "pub,function,,verify_sandwiches,1,14-75,486",
    "pub,class,ABC,CodeStripper,2,10-104,878",
    "prv,method,CodeStripper,__init__,2,12-19,68",
    "pub,method,CodeStripper,detect_single,2,21-24,36",
    "pub,method,CodeStripper,detect_multi_open,2,26-29,24",
    "pub,method,CodeStripper,detect_multi_close,2,31-34,37",
    "pub,method,CodeStripper,strip,2,36-104,687",
    "pub,class,CodeStripper,CodeStringStripper,2,106-166,740",
    "pub,class,CodeStripper,CodeCommentStripper,2,168-198,392",
    "pub,class,,ContentBlock,3,22-384,3724",
    "prv,method,ContentBlock,__init__,3,25-54,360",
    "pub,method,ContentBlock,parse_warn,3,56-59,27",
    "pub,method,ContentBlock,strip_strings,3,61-78,175",
    "pub,method,ContentBlock,strip_comments,3,80-94,136",
    "pub,method,ContentBlock,save_clean,3,96-108,144",
    "pub,method,ContentBlock,get_clean_content,3,110-121,105",
    "pub,method,ContentBlock,find_line,3,123-130,81",
    "pub,method,ContentBlock,count_chars,3,132-142,109",
    "pub,method,ContentBlock,sorted_entities,3,144-152,87",
    "pub,method,ContentBlock,detect_bounds,3,154-178,271",
    "pub,method,ContentBlock,check_entity_placement,3,180-200,240",
    "pub,method,ContentBlock,add_entity,3,202-217,190",
    "pub,method,ContentBlock,extract_entity_text,3,219-228,132",
    "pub,method,ContentBlock,extend_deps,3,230-241,118",
    "pub,method,ContentBlock,full_text_replace,3,243-273,371",
    "pub,method,ContentBlock,compress,3,275-360,973",
    "pub,method,ContentBlock,to_sandwich_block,3,362-381,156",
    "pub,method,ContentBlock,parse_content,3,383-384,30",
    "pub,class,EntityParser,DepsParser,4,11-36,226",
    "prv,method,DepsParser,__init__,4,13-16,58",
    "pub,method,DepsParser,add_module,4,18-22,44",
    "pub,method,DepsParser,add_import,4,24-28,47",
    "pub,method,DepsParser,store_deps,4,30-36,64",
    "pub,function,,organize_modules,4,39-100,629",
    "pub,local_function,,dfs,4,67-77,20",
    "pub,class,ContentBlock,DocumentBlock,5,9-30,173",
    "prv,method,DocumentBlock,__init__,5,12-20,105",
    "pub,method,DocumentBlock,parse_content,5,22-30,42",
    "pub,function,,match_value,6,11-14,53",
    "pub,function,,get_start_pos,6,17-18,53",
    "pub,class,,EntityParser,6,21-258,2486",
    "prv,method,EntityParser,__init__,6,24-46,340",
    "prv,method,EntityParser,_format_entity_name,6,48-55,88",
    "prv,method,EntityParser,_format_inner_name,6,57-58,26",
    "pub,method,EntityParser,make_entity,6,60-90,333",
    "pub,method,EntityParser,make_add_entity,6,92-115,273",
    "pub,method,EntityParser,detect_abstract,6,117-121,67",
    "pub,method,EntityParser,detect_visibility,6,123-124,32",
    "prv,method,EntityParser,_process_match,6,126-172,470",
    "pub,method,EntityParser,parse,6,174-191,152",
    "pub,method,EntityParser,parse_inner,6,193-243,559",
    "pub,method,EntityParser,masquerade,6,245-258,136",
    "pub,class,,IterativeRegex,7,8-113,846",
    "prv,method,IterativeRegex,__init__,7,11-14,27",
    "pub,method,IterativeRegex,add_token,7,16-31,170",
    "pub,method,IterativeRegex,all_matches,7,33-55,165",
    "pub,method,IterativeRegex,validate_match,7,57-113,474",
    "pub,class,EntityParser,ObjectParser,8,28-57,358",
    "prv,method,ObjectParser,__init__,8,30-34,87",
    "prv,method,ObjectParser,_format_entity_name,8,36-38,40",
    "pub,method,ObjectParser,parse,8,40-57,218",
    "pub,class,EntityParser,MethodParser,8,60-108,510",
    "pub,class,EntityParser,FunctionParser,8,111-141,371",
    "pub,class,EntityParser,InterfaceParser,8,144-151,116",
    "pub,class,EntityParser,ClassParser,8,154-161,114",
    "pub,class,DepsParser,DepsParserJs,8,164-188,254",
    "prv,method,DepsParserJs,_process_match,8,174-188,152",
    "pub,class,ContentBlock,ContentCodeJs,8,191-239,467",
    "pub,method,ContentCodeJs,parse_content,8,204-239,325",
    "pub,class,ContentCodeJs,ContentCodeTypeScript,8,242-288,443",
    "pub,function,,estimate_tokens,9,7-22,138",
    "pub,class,EntityParser,ClassParser,10,30-40,106",
    "prv,method,ClassParser,__init__,10,32-40,93",
    "pub,class,EntityParser,FunctionParser,10,43-46,54",
    "pub,class,DepsParser,DepsParserPHP,10,49-68,158",
    "prv,method,DepsParserPHP,_process_match,10,61-68,85",
    "pub,class,ContentBlock,ContentCodePHP,10,70-150,737",
    "pub,method,ContentCodePHP,strip_strings,10,91-105,96",
    "pub,method,ContentCodePHP,check_raw_escape,10,108-113,111",
    "pub,method,ContentCodePHP,parse_content,10,115-150,326",
    "pub,class,EntityParser,ClassParser,11,21-49,397",
    "prv,method,ClassParser,__init__,11,23-28,103",
    "pub,method,ClassParser,parse,11,30-49,281",
    "pub,class,EntityParser,FunctionParser,11,52-103,666",
    "prv,method,FunctionParser,_format_entity_name,11,61-62,21",
    "pub,class,DepsParser,DepsParserPython,11,106-127,194",
    "pub,class,ContentBlock,ContentCodePython,11,130-227,1040",
    "pub,method,ContentCodePython,detect_bounds,11,147-190,503",
    "pub,method,ContentCodePython,parse_content,11,192-227,327",
    "pub,class,EntityParser,ModuleParser,12,22-66,552",
    "prv,method,ModuleParser,__init__,12,24-28,88",
    "prv,method,ModuleParser,_process_match,12,30-66,451",
    "pub,class,EntityParser,TraitParser,12,69-83,199",
    "pub,class,EntityParser,TraitImplParser,12,86-109,261",
    "prv,method,TraitImplParser,_format_entity_name,12,104-109,56",
    "pub,class,EntityParser,FunctionParser,12,112-120,132",
    "pub,class,DepsParser,DepsParserRust,12,123-194,576",
    "pub,method,DepsParserRust,process_imports,12,145-183,365",
    "pub,class,ContentBlock,ContentCodeRust,12,199-279,787",
    "pub,method,ContentCodeRust,check_lines_match,12,214-227,171",
    "pub,method,ContentCodeRust,count_chars,12,229-238,109",
    "pub,method,ContentCodeRust,parse_content,12,240-279,343",
    "pub,function,,compute_md5,13,18-19,28",
    "pub,class,,SandwichPack,13,22-274,2379",
    "prv,method,SandwichPack,__init__,13,25-41,183",
    "pub,method,SandwichPack,register_block_class,13,43-46,32",
    "pub,method,SandwichPack,load_block_classes,13,48-63,160",
    "pub,method,SandwichPack,supported_type,13,65-72,84",
    "pub,class,EntityParser,FunctionParser,14,16-45,369",
    "prv,method,FunctionParser,__init__,14,18-25,80",
    "pub,method,FunctionParser,parse,14,27-45,275",
    "pub,class,DepsParser,DepsParserShell,14,48-61,127",
    "prv,method,DepsParserShell,_process_match,14,55-61,68",
    "pub,class,ContentBlock,ContentShellScript,14,64-104,425",
    "pub,method,ContentShellScript,parse_content,14,75-104,306",
    "pub,class,EntityParser,ComponentParser,15,17-39,286",
    "prv,method,ComponentParser,__init__,15,19-22,67",
    "pub,method,ComponentParser,parse,15,24-39,205",
    "pub,class,ContentBlock,ContentCodeVue,15,42-91,480",
    "pub,method,ContentCodeVue,parse_content,15,56-91,326",
    "pub,function,,dump_entities,17,20-24,46",
    "pub,class,,TestParsersBrief,17,27-38,103",
    "pub,method,TestParsersBrief,setUp,17,28-29,15",
    "pub,method,TestParsersBrief,entity_check,17,31-33,52",
    "pub,method,TestParsersBrief,test_rust_parser,17,35-38,24",
    "pub,function,,test_vue_parser,17,62-65,24",
    "pub,function,,test_shell_parser,17,87-90,24",
    "pub,function,,test_python_parser,17,107-110,25",
    "pub,function,,test_function,17,111-112,10",
    "pub,class,,TestClass,17,114-116,18",
    "pub,method,TestClass,test_method,17,115-116,12",
    "pub,function,,test_js_parser,17,130-133,24",
    "pub,function,,test_php_parser,17,165-168,24",
    "pub,function,,test_parse_modules,18,25-40,143",
    "pub,function,,test_protect_modules,18,43-62,100",
    "pub,function,,simpleFunction,19,5-7,16",
    "pub,function,,arrowFunction,19,10-12,22",
    "pub,function,,exprFunction,19,15-17,23",
    "pub,object,,myObject,19,20-26,31",
    "pub,method,myObject,myMethod,19,22-24,5",
    "pub,method,myObject,myComputed,19,31-33,5",
    "pub,object,,module,19,39-45,38",
    "pub,function,,incompleteString,19,42-45,26",
    "pub,object,,s,19,43-50,35",
    "pub,function,,simple_function,20,6-8,17",
    "pub,class,,MyClass,20,11-19,65",
    "pub,method,MyClass,my_method,20,12-14,11",
    "pub,method,MyClass,sqli,20,15-17,8",
    "pub,abstract method,MyClass,my_abstract_method,20,18-18,12",
    "pub,function,,last_function,20,29-30,20",
    "pub,function,,simple_function,21,5-9,28",
    "pub,local_function,,local_inner,21,6-7,11",
    "pub,class,,MyClass,21,12-18,32",
    "pub,method,MyClass,my_method,21,13-14,11",
    "pub,method,MyClass,my_classmethod,21,16-18,12",
    "prv,function,,simple_function,22,3-6,27",
    "prv,function,,raw_string_function,22,9-12,30",
    "prv,function,,comment_function,22,15-18,30",
    "prv,function,,single_comment_function,22,21-24,20",
    "prv,structure,,Outer,22,27-29,17",
    "prv,structure,,Inner,22,31-33,13",
    "prv,function,,incomplete_string,22,36-39,27",
    "prv,interface,Send + Sync,ExampleTrait,22,42-44,34",
    "prv,method,ExampleTrait,trait_method,22,43-50,14",
    "prv,class,,ExampleTrait<Outer>,22,46-50,38",
    "pub,module,,logger,22,53-57,29",
    "prv,function,,logger.logger_function,22,54-56,19",
    "pub,module,,extra_module,22,60-67,47",
    "prv,structure,,extra_module.ExtraStruct,22,61-63,18",
    "prv,function,,extra_module.extra_function,22,64-66,18",
    "prv,class,,Inner,22,69-74,33",
    "prv,async method,Inner,new,22,70-73,15",
    "prv,interface,,LoadEquityData,22,86-94,78",
    "prv,async method,LoadEquityData,load_equity_data,22,87-97,64",
    "prv,class,,LoadEquityData<MySqlDataSource>,22,98-180,741",
    "pub,interface,,MyInterface,23,4-6,18",
    "pub,class,,MyClass,23,8-12,24",
    "pub,function,,code_block,24,17-30,165",
    "prv,function,,_scan_log,24,33-36,47",
    "pub,class,,TestJsParse,24,39-139,991",
    "pub,method,TestJsParse,setUp,24,40-49,129",
    "pub,method,TestJsParse,test_parse_content,24,51-94,368",
    "pub,method,TestJsParse,test_incomplete_cases,24,96-104,82",
    "pub,method,TestJsParse,test_js_template_string_no_escape,24,106-121,162",
    "pub,method,TestJsParse,test_typescript_specific,24,124-139,239",
    "pub,function,,entity_dump,25,13-14,26",
    "pub,function,,code_block,25,17-32,131",
    "pub,class,,TestPhpParse,25,34-68,339",
    "pub,method,TestPhpParse,setUp,25,35-41,75",
    "pub,method,TestPhpParse,test_parse_content,25,43-68,253",
    "pub,function,,code_block,26,17-30,140",
    "pub,class,,TestPyParse,26,32-92,577",
    "pub,method,TestPyParse,setUp,26,33-42,116",
    "pub,method,TestPyParse,test_parse_content,26,44-92,450",
    "pub,function,,code_block,27,17-26,65",
    "prv,function,,_scan_log,27,29-32,47",
    "pub,class,,TestRustParse,27,35-107,647",
    "pub,method,TestRustParse,setUp,27,36-45,117",
    "pub,method,TestRustParse,test_parse_content,27,47-91,378",
    "pub,method,TestRustParse,test_rust_raw_string_no_escape,27,93-107,140"
  ],
  "files": [
    "0,/spack.py,75888932e5c12e41714e582e860150fc,1416,2025-08-07 08:36:29Z",
    "1,/spack_verify.py,36fa555d1a158aeac89f25fe455985d5,1159,2025-08-06 15:48:54Z",
    "2,/lib/code_stripper.py,e6a65dcd56b198354c017c25b6dff95b,2851,2025-08-06 10:30:03Z",
    "3,/lib/content_block.py,b10ee1f821550623fd1633fca4d3b4e7,5672,2025-08-08 15:21:37Z",
    "4,/lib/deps_builder.py,99307811effa026d03af424844d50339,1203,2025-08-08 13:25:05Z",
    "5,/lib/document_block.py,6e968595e8865f1e50c5bd452d8cce6c,328,2025-07-15 12:56:33Z",
    "6,/lib/entity_parser.py,50993ce1b4a16ad954680eea585bd281,3441,2025-08-09 16:28:14Z",
    "7,/lib/iter_regex.py,a40f86df51088fe4cc83f867a6006bb8,1313,2025-08-09 14:31:00Z",
    "8,/lib/js_block.py,67a21d31c69b0ad81a7c92d99809d637,3906,2025-08-09 15:41:29Z",
    "9,/lib/llm_tools.py,203fafd667304f7b3220e0ef5acb5218,208,2025-08-05 09:55:32Z",
    "10,/lib/php_block.py,e1681f6a7e571aac8a6aed107bbe1085,1824,2025-08-09 16:38:40Z",
    "11,/lib/python_block.py,d97712bc7786c74c9082b908adf197ef,3145,2025-08-09 15:31:43Z",
    "12,/lib/rust_block.py,7c7f7b2e335a1ba1fb9b7e857d6812fd,3713,2025-08-08 15:08:42Z",
    "13,/lib/sandwich_pack.py,a0f5aefc26ba4eb1a4f2b45a514e23a2,3304,2025-08-08 06:09:47Z",
    "14,/lib/shellscript_block.py,5098250db40d78a1f024516a76c66acf,1400,2025-08-09 15:21:01Z",
    "15,/lib/vue_block.py,f23a4130b724632fe047a36dcf8362c9,1302,2025-08-09 15:43:36Z",
    "16,/lib/__init__.py,074cf55a834c54634445ffec1561efc5,20,2025-07-15 06:32:56Z",
    "17,/tests/brief_tests.py,e6e22245ddf66bf84452ccf19c84c121,2122,2025-08-09 15:16:19Z",
    "18,/tests/regex_imports_php.py,570e7cea2bb6495deb1a3ddeb96be304,718,2025-08-03 08:01:14Z",
    "19,/tests/test.js,3f4fb5b0b76ccf9e32b7d0fd2fc90f63,355,2025-08-02 09:04:08Z",
    "20,/tests/test.php,216fe13e8200d647134d45b633e5aa49,205,2025-08-06 09:54:23Z",
    "21,/tests/test.py,e66e2fecf8c51c71fa539e202f5787ec,133,2025-08-04 09:40:17Z",
    "22,/tests/test.rs,535d66412b561d4854d66d846b39f452,1691,2025-08-08 15:00:13Z",
    "23,/tests/test.ts,b381efcb43350bd42bb39fb49de0a79a,81,2025-08-02 06:22:30Z",
    "24,/tests/test_js_parse.py,cdf881a06858c32d39d834874bd389ff,1999,2025-08-02 08:08:27Z",
    "25,/tests/test_php_parse.py,6b92921d34ff70d097997cee025fd276,855,2025-08-09 15:47:40Z",
    "26,/tests/test_py_parse.py,74980d75b14cff997096353ba425281a,1230,2025-08-04 08:28:15Z",
    "27,/tests/test_rust_parse.py,c4aaa2335248a4dfdce41e6c9e404de0,1294,2025-08-08 11:25:19Z"
  ],
  "users": []
}