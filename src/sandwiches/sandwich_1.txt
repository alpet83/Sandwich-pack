<python src="/spack.py" mod_time="2025-07-21 15:41:30Z">
# /spack.py, updated 2025-07-15 09:57 EEST
import os
import datetime
import logging
import argparse
from pathlib import Path
from lib.sandwich_pack import SandwichPack

logging.basicConfig(level=logging.DEBUG, format='%(asctime)s #%(levelname)s: %(message)s')

def get_file_mod_time(file_path):
    mtime = os.path.getmtime(file_path)
    mod_time = datetime.datetime.fromtimestamp(mtime, datetime.UTC)
    return mod_time.strftime("%Y-%m-%d %H:%M:%SZ")

def is_hidden_file(filepath):
    return any(part.startswith(".") for part in filepath.parts)

def collect_files(root_dir):
    content = []
    root_path = Path(root_dir).parent
    logging.debug(f"Scanning directory: {root_dir}")
    if not os.path.exists(root_dir):
        logging.error(f"Directory {root_dir} does not exist")
        return content
    for file_path in Path(root_dir).rglob("*"):
        if file_path.is_file() and not is_hidden_file(file_path):
            relative_path = f"/{file_path.relative_to(root_path)}".replace("\\", "/")
            extension = Path(file_path).suffix.lower()
            content_type = extension if extension else ""
            if not content_type or not SandwichPack.supported_type(content_type):
                logging.debug(f"Skipping unsupported content_type: {content_type} for {relative_path}")
                continue
            try:
                with open(file_path, "r", encoding="utf-8-sig", errors="replace") as f:
                    text = f.read()
                logging.debug(f"Read {file_path} with encoding: utf-8-sig")
            except UnicodeDecodeError as e:
                logging.warning(f"Non-UTF-8 characters in {file_path}, replaced with �: {e}")
                continue
            mod_time = get_file_mod_time(file_path)
            logging.debug(f"Collected file: {relative_path} with content_type: {content_type}")
            content.append(SandwichPack.create_block(
                content_text=text,
                content_type=content_type,
                file_name=relative_path,
                timestamp=mod_time
            ))
    for file_path in root_path.glob("*.toml"):
        if not is_hidden_file(file_path):
            relative_path = f"/{file_path.name}".replace("\\", "/")
            content_type = ".toml"
            if not SandwichPack.supported_type(content_type):
                logging.debug(f"Skipping unsupported content_type: {content_type} for {relative_path}")
                continue
            try:
                with open(file_path, "r", encoding="utf-8-sig", errors="replace") as f:
                    text = f.read()
                logging.debug(f"Read {file_path} with encoding: utf-8-sig")
            except UnicodeDecodeError as e:
                logging.warning(f"Non-UTF-8 characters in {file_path}, replaced with �: {e}")
                continue
            mod_time = get_file_mod_time(file_path)
            logging.debug(f"Collected file: {relative_path} with content_type: {content_type}")
            content.append(SandwichPack.create_block(
                content_text=text,
                content_type=content_type,
                file_name=relative_path,
                timestamp=mod_time
            ))
    return content

def main():
    logging.info("Starting spack CLI")
    SandwichPack.load_block_classes()
    project_dir = "."
    output_dir = "./sandwiches"
    files_content = collect_files(project_dir)
    if not files_content:
        logging.error("No files collected, exiting")
        raise SystemExit("Error: No files found in the specified directory")
    logging.info(f"Collected {len(files_content)} files")
    parser = argparse.ArgumentParser(
        prog='Sandwich Packer',
        description='Combining all project sources files into sandwich structured several text files',
        epilog='Best for using with chatbots like Grok or ChatGPT')

    parser.add_argument('project_name')
    args = parser.parse_args()
    packer = SandwichPack(args.project_name, max_size=80_000)
    result = packer.pack(files_content)
    os.makedirs(output_dir, exist_ok=True)
    for i, sandwich in enumerate(result["sandwiches"], 1):
        output_file = Path(output_dir) / f"sandwich_{i}.txt"
        with open(output_file, "w", encoding="utf-8") as f:
            f.write(sandwich)
        logging.info(f"Created {output_file} ({len(sandwich.encode('utf-8'))} bytes)")
    global_index_file = Path(output_dir) / "sandwiches_index.json"
    with open(global_index_file, "w", encoding="utf-8") as f:
        f.write(result["index"])
    global_index_file = Path(output_dir) / "sandwiches_structure.json"
    with open(global_index_file, "w", encoding="utf-8") as f:
        f.write(result["deep_index"])

    logging.info(f"Created {global_index_file}")

if __name__ == "__main__":
    main()

</python>
<python src="/lib/content_block.py" mod_time="2025-08-01 08:59:48Z">
# /lib/content_block.py, updated 2025-07-31 14:11 EEST
# Formatted with proper line breaks and indentation for project compliance.

import logging
import re
import os
import math
from pathlib import Path

# CRITICAL NOTICE: Using content for bounds detection before full cleaning is prohibited to avoid errors from comments/strings.
# PROTECTION CODE DON'T TOUCH!!!
Optional = None
List = None
Dict = None
Tuple = None
assert List is None
assert Dict is None
assert Tuple is None

logging.basicConfig(
    level=os.environ.get('LOGLEVEL', 'DEBUG').upper()
)

def estimate_tokens(content):
    """Estimates tokens by counting words and spaces more accurately."""
    if not content:
        return 0
    tokens = 0
    words = re.findall(r'\S+', content)
    for word in words:
        if len(word) >= 5:
            tokens += math.ceil(len(word) / 4)
        else:
            tokens += 1
    spaces = len(re.findall(r'\s+', content))
    tokens += spaces
    logging.debug("Estimated tokens for content (length=%d): %d tokens (words=%d, spaces=%d)",
                  len(content), tokens, len(words), spaces)
    return tokens

class ContentBlock:
    supported_types = [':document', ':post']

    def __init__(self, content_text, content_type, file_name=None, timestamp=None, **kwargs):
        self.content_text = content_text
        self.content_type = content_type
        self.tag = "post" if content_type == ":post" else "document"
        self.file_name = file_name
        self.timestamp = timestamp
        self.post_id = kwargs.get('post_id')
        self.user_id = kwargs.get('user_id')
        self.relevance = kwargs.get('relevance', 0)
        self.file_id = kwargs.get('file_id')
        self.tokens = estimate_tokens(content_text)
        self.clean_lines = ["Line №0"] + self.content_text.splitlines()  # 1-based indexing, volatile
        self.strip_log = []  # Log of detected comments and strings for debugging
        self.warnings = []  # List of warning messages
        self.entity_map = {}  # Dict[first_line: entity]
        self.string_quote_chars = "\"'"  # Characters for string literals
        self.raw_str_prefix = None  # Prefix for raw strings (e.g., 'r' for Rust)
        self.raw_quote_char = None  # Quote char for raw strings (e.g., "'" for PHP)
        self.open_ml_string = []  # Opening sequences for multi-line strings
        self.close_ml_string = []  # Closing sequences for multi-line strings
        self.open_sl_comment = ["//"]  # Single-line comment starts
        self.open_ml_comment = ["/*"]  # Multi-line comment starts
        self.close_ml_comment = ["*/"]  # Multi-line comment ends
        self.escape_char = "\\"  # Escape character
        self.module_prefix = ""  # Tracks current module prefix (e.g., "logger.")
        self.line_offsets = []  # List of character offsets for each line in clean_content
        logging.debug(f"Initialized ContentBlock with content_type={content_type}, tag={self.tag}, file_name={file_name}")

    def parse_warn(self, msg):
        """Logs a warning and adds it to self.warnings."""
        self.warnings.append(msg)
        logging.warning(msg)

    def check_raw_escape(self, line, position, quote_char):
        """Checks if the character at position is part of a raw string escape sequence."""
        return False  # Default: no escape sequences in raw strings (e.g., Rust)

    def strip_raw_strings(self):
        """Strips raw string literals, preserving empty lines."""
        if len(self.clean_lines) <= 1:
            raise Exception("clean_lines not filled")
        clean_lines = self.clean_lines
        result_lines = [""]
        in_raw_string = False
        quote_char = None
        _rsq_len = len(self.raw_str_prefix) if self.raw_str_prefix else 0
        for line_num, line in enumerate(clean_lines[1:], 1):
            if not isinstance(line, str):
                result_lines.append("")
                continue
            clean_line = ""
            i = 0
            _len = len(line)
            while i < _len:
                char = line[i]
                if not in_raw_string:
                    prefix_chars = line[i - _rsq_len:i] if i >= _rsq_len else None
                    _is_raw_start = self.raw_str_prefix and prefix_chars == self.raw_str_prefix
                    _is_quote_char = char in self.string_quote_chars
                    if (self.raw_quote_char and char == self.raw_quote_char) or (_is_raw_start and _is_quote_char):
                        in_raw_string = True
                        quote_char = char
                        i += _rsq_len if _is_raw_start else 1
                        self.strip_log.append(f"Raw string {'with prefix ' + self.raw_str_prefix if _is_raw_start else ''} started at line {line_num}, pos {i}, quote: '{char}', line: '{line}'")
                    clean_line += char
                elif self.check_raw_escape(line, i, quote_char):
                    i += 1
                elif char == quote_char:
                    in_raw_string = False
                    quote_char = None
                    clean_line += char
                    self.strip_log.append(f"Raw string ended at line {line_num}, pos {i + 1}, line: '{line}'")
                i += 1
            result_lines.append(clean_line)
            if in_raw_string:
                self.parse_warn(f"Incomplete raw string literal in file {self.file_name} at line {line_num}")
                self.strip_log.append(f"Incomplete raw string at line {line_num}, line: '{line}'")
        self.clean_lines = result_lines
        return result_lines

    def strip_strings(self):
        """Strips string literals with full escaping, preserving empty lines."""
        if len(self.clean_lines) <= 1:
            self.clean_lines = [''] + self.content_text.splitlines()
        self.strip_raw_strings()
        self.strip_multiline_strings()
        clean_lines = self.clean_lines
        result_lines = [""]
        quote_char = None
        for line_num, line in enumerate(clean_lines[1:], 1):
            in_string = False
            if not isinstance(line, str):
                result_lines.append("")
                continue
            clean_line = ""
            i = 0
            _len = len(line)
            while i < _len:
                char = line[i]
                if not in_string:
                    if char in self.string_quote_chars:
                        in_string = True
                        quote_char = char
                    clean_line += char
                elif char == self.escape_char and i + 1 < _len:
                    i += 1
                elif char == quote_char:
                    in_string = False
                    quote_char = None
                    clean_line += char
                i += 1
            result_lines.append(clean_line)
            if in_string:
                self.parse_warn(f"Incomplete string literal in file {self.file_name} at line {line_num}")
                self.strip_log.append(f"Incomplete string at line {line_num}, line: '{line}'")
        self.clean_lines = result_lines
        return result_lines

    def strip_multiline_strings(self):
        """Strips multi-line string literals, preserving empty lines."""
        if len(self.clean_lines) <= 1:
            raise Exception("clean_lines not filled")
        clean_lines = self.clean_lines
        result_lines = [""]
        in_multi_string = False
        multi_quote = None
        for line_num, line in enumerate(clean_lines[1:], 1):
            if not isinstance(line, str):
                result_lines.append("")
                continue
            clean_line = ""
            i = 0
            _len = len(line)
            while i < _len:
                char = line[i]
                if not in_multi_string:
                    for open_quote in self.open_ml_string:
                        _oq_len = len(open_quote)
                        if i + _oq_len <= _len and line[i:i+_oq_len] == open_quote:
                            in_multi_string = True
                            multi_quote = open_quote
                            clean_line += open_quote
                            i += _oq_len - 1
                            self.strip_log.append(f"Multi-line string started at line {line_num}, pos {i + 1}, quote: '{open_quote}', line: '{line}'")
                            break
                    else:
                        clean_line += char
                elif i + len(self.close_ml_string[self.open_ml_string.index(multi_quote)]) <= _len and \
                     line[i:i+len(self.close_ml_string[self.open_ml_string.index(multi_quote)])] == self.close_ml_string[self.open_ml_string.index(multi_quote)]:
                    close_quote = self.close_ml_string[self.open_ml_string.index(multi_quote)]
                    in_multi_string = False
                    multi_quote = None
                    clean_line += close_quote
                    i += len(close_quote) - 1
                    self.strip_log.append(f"Multi-line string ended at line {line_num}, pos {i + 1}, line: '{line}'")
                i += 1
            result_lines.append(clean_line)
            if in_multi_string:
                self.parse_warn(f"Incomplete multi-line string in file {self.file_name} at line {line_num}")
                self.strip_log.append(f"Incomplete multi-line string at line {line_num}, line: '{line}'")
        self.clean_lines = result_lines
        return result_lines

    def strip_comments(self):
        """Strips comments from content, preserving empty lines."""
        if len(self.clean_lines) <= 1:
            raise Exception("clean_lines not filled")
        clean_lines = self.clean_lines.copy()
        result_lines = [""]
        in_multi_comment = False
        multi_comment_open = None
        for line_num, line in enumerate(clean_lines[1:], 1):
            if not isinstance(line, str):
                result_lines.append("")
                continue
            clean_line = ""
            i = 0
            _len = len(line)
            if in_multi_comment:
                end_pos = line.find(self.close_ml_comment[self.open_ml_comment.index(multi_comment_open)])
                if end_pos >= 0:
                    in_multi_comment = False
                    multi_comment_open = None
                    clean_line = line[end_pos + len(self.close_ml_comment[0]):]
                    self.strip_log.append(f"Multi-line comment ended at line {line_num}, pos {end_pos}, remaining: '{clean_line}', line: '{line}'")
                else:
                    clean_line = ""
                    self.strip_log.append(f"Multi-line comment continued at line {line_num}, line: '{line}'")
                    result_lines.append(clean_line)
                    continue
            while i < _len:
                char = line[i]
                min_start_pos = -1
                min_start_type = None
                min_comment = None
                for sl_comment in self.open_sl_comment:
                    start_pos = line.find(sl_comment, i)
                    if start_pos >= 0 and (min_start_pos < 0 or start_pos < min_start_pos):
                        min_start_pos = start_pos
                        min_start_type = "single"
                        min_comment = sl_comment
                for ml_comment in self.open_ml_comment:
                    multi_pos = line.find(ml_comment, i)
                    if multi_pos >= 0 and (min_start_pos < 0 or multi_pos < min_start_pos):
                        min_start_pos = multi_pos
                        min_start_type = "multi"
                        min_comment = ml_comment
                if min_start_type == "single":
                    clean_line += line[i:min_start_pos]
                    self.strip_log.append(f"Single-line comment at line {line_num}, pos {min_start_pos}, stripped: '{clean_line}', line: '{line}'")
                    break
                elif min_start_type == "multi":
                    clean_line += line[i:min_start_pos]
                    in_multi_comment = True
                    multi_comment_open = min_comment
                    i += len(multi_comment_open)
                    end_pos = line.find(self.close_ml_comment[self.open_ml_comment.index(multi_comment_open)], i)
                    self.strip_log.append(f"Multi-line comment started at line {line_num}, pos {min_start_pos}, line: '{line}'")
                    if end_pos >= 0:
                        in_multi_comment = False
                        multi_comment_open = None
                        clean_line += line[end_pos + len(self.close_ml_comment[0]):]
                        self.strip_log.append(f"Multi-line comment ended at line {line_num}, pos {end_pos}, remaining: '{clean_line}', line: '{line}'")
                        i = end_pos + len(self.close_ml_comment[0])
                    else:
                        break
                else:
                    clean_line += char
                i += 1
            result_lines.append(clean_line)
            if in_multi_comment and line_num == len(clean_lines) - 1:
                self.parse_warn(f"Incomplete multi-line comment in file {self.file_name} at line {line_num}")
                self.strip_log.append(f"Incomplete multi-line comment at line {line_num}, line: '{line}'")
                for j in range(line_num + 1, len(clean_lines)):
                    result_lines.append("")
                    self.strip_log.append(f"Multi-line comment continued at line {j}, line: '{clean_lines[j]}'")
        self.clean_lines = result_lines
        logging.debug(f"After strip_comments, lines 60-70: {self.clean_lines[60:71]}")
        return result_lines

    def save_clean(self, file_name):
        """Saves the cleaned content to a file for debugging, replacing empty lines with line number comments."""
        if len(self.clean_lines) <= 1:
            raise Exception("clean_lines not filled")
        try:
            output_path = Path(file_name)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            with output_path.open("w", encoding="utf-8") as f:
                for line_num, line in enumerate(self.clean_lines[1:], 1):
                    f.write((line if line.strip() else f"// Line {line_num}") + "\n")
            logging.debug(f"Saved cleaned content to {file_name}")
        except Exception as e:
            logging.error(f"Failed to save cleaned content to {file_name}: {str(e)}")

    def get_clean_content(self):
        """Returns the cleaned content as a single string and updates line_offsets."""
        if len(self.clean_lines) <= 1:
            raise Exception("clean_lines not initialized")
        content = "\n".join(self.clean_lines[1:])
        self.line_offsets = [0]
        offset = 0
        for line in self.clean_lines[1:]:
            offset += len(line) + 1  # +1 for newline
            self.line_offsets.append(offset)
        logging.debug(f"Updated line_offsets: {self.line_offsets[:10]}... (total {len(self.line_offsets)})")
        logging.debug(f"Line offsets 46-61: {self.line_offsets[46:62]}")
        return content

    def find_line(self, content_offset):
        """Finds the line number for a given content offset."""
        if not self.line_offsets:
            self.get_clean_content()  # Ensure line_offsets is populated
        for i, offset in enumerate(self.line_offsets):
            if content_offset < offset:
                return i
        return len(self.line_offsets) - 1

    def count_chars(self, line_num, ch):
        """Counts occurrences of a character in a specific line of clean code."""
        if len(self.clean_lines) <= 1:
            self.strip_strings()
            self.strip_comments()
        if line_num < 1 or line_num >= len(self.clean_lines):
            logging.error(f"Invalid line number {line_num} for file {self.file_name}")
            return 0
        line = self.clean_lines[line_num]
        if not isinstance(line, str):
            return 0
        return line.count(ch)

    def sorted_entities(self):
        """Sorts entities by their line number."""
        sorted_map = {}
        result = []
        for line_num in sorted(self.entity_map.keys()):
            sorted_map[line_num] = self.entity_map[line_num]
            result.append(self.entity_map[line_num])
        self.entity_map = sorted_map
        return result

    def detect_bounds(self, start_line, clean_lines: list):
        """Detects the start and end line of an entity using brace counting."""
        if start_line < 1 or start_line >= len(clean_lines) or not clean_lines[start_line] or not clean_lines[start_line].strip():
            logging.error(f"Invalid start line {start_line} for file {self.file_name} module [{self.module_prefix}]")
            return start_line, start_line
        brace_count = 0
        line_num = start_line
        while line_num < len(clean_lines):
            line = clean_lines[line_num]
            if not isinstance(line, str) or not line.strip():
                line_num += 1
                continue
            brace_count += line.count('{') - line.count('}')
            logging.debug(f"Line {line_num}: brace_count={brace_count}, line: '{line.strip()}'")
            if brace_count == 0 and line_num >= start_line:
                # Include the line with the closing brace
                return start_line, line_num
            line_num += 1
        self.parse_warn(f"Incomplete entity at line {start_line} in file {self.file_name}, brace_count={brace_count}")
        return start_line, start_line

    def check_entity_placement(self, line_num: int, name: str):
        """Checks if an entity with the given name is correctly placed at line_num."""
        if line_num < 1 or line_num >= len(self.clean_lines) or not self.clean_lines[line_num]:
            return False
        line = self.clean_lines[line_num]
        base_name = name.split(".")[-1]  # Get in module name
        base_name = base_name.split("::")[-1].split("<")[0]  # Extract base name

        pattern = rf"\b{base_name}\b"
        result = bool(re.search(pattern, line))
        if not result:
            # Search for first occurrence of base_name in clean_lines
            for i, search_line in enumerate(self.clean_lines[1:], 1):
                if isinstance(search_line, str) and re.search(pattern, search_line):
                    logging.debug(f"First occurrence of '{base_name}' found at line {i}: '{search_line}'")
                    break
            else:
                logging.debug(f"No occurrence of '{base_name}' found in clean_lines")
        logging.debug(f"Checking entity placement for {name} at line {line_num}: {'Passed' if result else 'Failed'}, line: '{line}'")
        return result

    def check_lines_match(self, offset, full_clean_lines):
        """Validates that clean_lines matches full_clean_lines at the given offset."""
        if offset < 1 or offset >= len(self.clean_lines):
            logging.error(f"Invalid offset {offset} for file {self.file_name}")
            return False
        for i, line in enumerate(self.clean_lines[offset:], offset):
            if i >= len(full_clean_lines):
                return False
            if line != full_clean_lines[i]:
                logging.warning(f"Line mismatch at {i}: expected '{full_clean_lines[i]}', got '{line}'")
                return False
        return True

    def add_entity(self, line_num: int, entity: dict):
        """Adds an entity to entity_map with placement and duplication checks."""
        if line_num in self.entity_map:
            existing = self.entity_map[line_num]
            if existing["name"] != entity["name"] or existing["type"] != entity["type"]:
                logging.warning(f"Duplicate entity at line {line_num}: {entity['name']} ({entity['type']}) conflicts with {existing['name']} ({existing['type']})")
                return False
        if not self.check_entity_placement(line_num, entity["name"]):
            logging.warning(f"Entity {entity['name']} placement check failed at line {line_num}, line: '{self.clean_lines[line_num]}'")
            return False
        entity["first_line"] = line_num
        # Respect last_line for abstract methods, otherwise use detect_bounds
        if entity["type"] != "abstract method" or "last_line" not in entity:
            entity["last_line"] = self.detect_bounds(line_num, self.clean_lines)[1]
        self.entity_map[line_num] = entity
        logging.debug(f"Added entity {entity['name']} at first_line={line_num}, last_line={entity['last_line']}")
        return True

    def _extract_full_entity(self, start, end_header, content=None):
        """Extracts the full entity text using clean_lines for brace counting."""
        if len(self.clean_lines) <= 1:
            raise Exception("clean_lines not filled")
        content = content or self.get_clean_content()
        start_pos = start
        lines = content.splitlines()
        start_line = self.find_line(start_pos)
        logging.debug(f"Calculating bounds for start_pos={start_pos}, start_line={start_line}, content preview: {content[start:start + 100]}...")
        logging.debug(f"Entity at line {start_line}: '{self.clean_lines[start_line]}', raw: '{lines[start_line - 1]}'")
        logging.debug(f"Clean lines: {self.clean_lines[start_line-1:start_line+2]}")
        start_line, end_line = self.detect_bounds(start_line, self.clean_lines)
        if start_line == end_line:
            self.parse_warn(f"Incomplete entity in file {self.file_name} at start={start}, using header end")
            return content[start:end_header]
        logging.info(f"Extracted entity from first_line={start_line} to last_line={end_line}")
        return "\n".join(self.clean_lines[start_line:end_line + 1])

    def to_sandwich_block(self):
        attrs = []
        if self.content_type == ":post":
            attrs.append(f'post_id="{self.post_id}"')
            if self.user_id is not None:
                attrs.append(f'user_id="{self.user_id}"')
            if self.timestamp:
                attrs.append(f'mod_time="{self.timestamp}"')
            if self.relevance is not None:
                attrs.append(f'relevance="{self.relevance}"')
        else:
            if self.file_name:
                attrs.append(f'src="{self.file_name}"')
            if self.timestamp:
                attrs.append(f'mod_time="{self.timestamp}"')
            if self.file_id is not None:
                attrs.append(f'file_id="{self.file_id}"')
        attr_str = " ".join(attrs)
        return f"<{self.tag} {attr_str}>\n{self.content_text}\n</{self.tag}>"
</python>
<python src="/lib/document_block.py" mod_time="2025-07-15 12:56:33Z">
# /lib/document_block.py, created 2025-07-15 15:53 EEST
import logging
from typing import Dict
from lib.content_block import ContentBlock
from lib.sandwich_pack import SandwichPack

logging.basicConfig(level=logging.DEBUG, format='%(asctime)s #%(levelname)s: %(message)s')

class DocumentBlock(ContentBlock):
    supported_types = ['.md', '.conf', '.toml', '.rulz']

    def __init__(self, content_text: str, content_type: str, file_name: str, timestamp: str, **kwargs):
        super().__init__(content_text, content_type, file_name, timestamp, **kwargs)
        self.tag = {
            '.md': 'markdown',
            '.conf': 'conf',
            '.toml': 'toml',
            '.rulz': 'rules'
        }.get(content_type, 'document')
        logging.debug(f"Initialized DocumentBlock with tag={self.tag}, content_type={content_type}, file_name={file_name}")

    def parse_content(self) -> Dict:
        return {
            "entities": [],
            "dependencies": {
                "imports": [],
                "modules": [],
                "calls": []
            }
        }

SandwichPack.register_block_class(DocumentBlock)
</python>
<python src="/lib/js_block.py" mod_time="2025-08-01 10:06:34Z">
# /lib/js_block.py, updated 2025-08-01 13:10 EEST
# Formatted with proper line breaks and indentation for project compliance.

import re
import logging
from typing import Dict, List
from lib.content_block import ContentBlock, estimate_tokens
from lib.sandwich_pack import SandwichPack

class ContentCodeJs(ContentBlock):
    supported_types = [".js"]

    def __init__(self, content_text: str, content_type: str, file_name: str, timestamp: str, **kwargs):
        super().__init__(content_text, content_type, file_name, timestamp, **kwargs)
        self.tag = "jss"
        self.string_quote_chars = "\"'`"
        self.open_ml_string = ["`"]
        self.close_ml_string = ["`"]
        self.entity_map = {}  # Use entity_map for consistency with Rust
        logging.debug(f"Initialized ContentCodeJs with tag={self.tag}, file_name={file_name}")

    def parse_content(self) -> Dict:
        """Parses JavaScript content to extract entities and dependencies."""
        self.entity_map = {}
        dependencies = {"modules": [], "imports": [], "calls": []}
        clean_content = self.get_clean_content()
        lines = self.clean_lines
        content_offset = 1
        object_context = None
        object_indent = None

        # Initialize clean_lines
        self.strip_strings()
        self.strip_comments()

        # Find objects (e.g., export default { ... })
        object_pattern = re.compile(
            r"^(?P<indent>[ \t]*)(?:export\s+default\s+|const\s+(?P<name>\w+)\s*=\s*)\s*{\s*",
            re.MULTILINE
        )
        for match in object_pattern.finditer(clean_content):
            start_pos = match.start()  # Use start of match for correct line
            line_count = clean_content[:start_pos].count('\n')
            start_line = content_offset + line_count
            if start_line in self.entity_map:
                continue
            object_name = match.group('name')  # Use captured name
            if not object_name:
                logging.warning(f"No name captured for object at line {start_line}, skipping")
                continue
            full_text = self._extract_full_entity(match.start(), match.end(), clean_content)
            entity = {
                "type": "object",
                "name": object_name,
                "visibility": "public",
                "file_id": self.file_id,
                "first_line": start_line,
                "tokens": estimate_tokens(full_text)
            }
            self.add_entity(start_line, entity)
            object_indent = len(match.group('indent'))
            object_context = object_name
            logging.debug(f"Parsed object {object_name} at line {start_line}")

        # Find methods and functions
        method_pattern = re.compile(
            r"^(?P<indent>[ \t]*)(?:methods|computed|watch)\s*:\s*{\s*[^}]*\b(?P<name>\w+)\s*\(\s*\)\s*{",
            re.DOTALL | re.MULTILINE
        )
        fn_pattern = re.compile(
            r"^(?P<indent>[ \t]*)(?:function\s+(?P<name>\w+)\s*\(\s*\)\s*{|const\s+\w+\s*=\s*(?:async\s+)?function\s*\w+\s*\(\s*\)\s*{|const\s+\w+\s*=\s*\([^)]*\)\s*=>\s*{)",
            re.DOTALL | re.MULTILINE
        )
        for match in method_pattern.finditer(clean_content):
            indent = len(match.group('indent'))
            method_name = match.group('name')
            start_line = clean_content[:match.start('name')].count('\n') + content_offset
            if start_line in self.entity_map:
                continue
            full_text = self._extract_full_entity(match.start(), match.end(), clean_content)
            entity = {
                "type": "method",
                "name": f"{object_context}::{method_name}" if object_context else method_name,
                "visibility": "public",
                "file_id": self.file_id,
                "first_line": start_line,
                "tokens": estimate_tokens(full_text)
            }
            self.add_entity(start_line, entity)
            logging.debug(f"Parsed method {method_name} at line {start_line}")

        for match in fn_pattern.finditer(clean_content):
            indent = len(match.group('indent'))
            fn_name = match.group('name')
            start_line = clean_content[:match.start('name')].count('\n') + content_offset
            if start_line in self.entity_map:
                continue
            full_text = self._extract_full_entity(match.start(), match.end(), clean_content)
            ent_type = "method" if object_context and indent > object_indent else "function"
            name = f"{object_context}::{fn_name}" if ent_type == "method" else fn_name
            entity = {
                "type": ent_type,
                "name": name,
                "visibility": "public",
                "file_id": self.file_id,
                "first_line": start_line,
                "tokens": estimate_tokens(full_text)
            }
            self.add_entity(start_line, entity)
            logging.debug(f"Parsed {ent_type} {name} at line {start_line}")

        # Check object end
        for i, line in enumerate(lines[1:], 1):
            if not isinstance(line, str) or not line.strip():
                continue
            indent = len(line) - len(line.lstrip())
            if object_context and indent <= object_indent:
                object_context = None
                object_indent = None

        # Parse imports
        import_pattern = re.compile(
            r"import\s+{?([\w,\s]+)}?\s+from\s+['\"]([^'\"]+)['\"]|require\s*\(['\"]([^'\"]+)['\"]\)",
            re.MULTILINE
        )
        for match in import_pattern.finditer(clean_content):
            items = [item.strip() for item in match.group(1).split(",")] if match.group(1) else []
            module = match.group(2) or match.group(3)
            for item in items:
                dependencies["imports"].append(item)
            if module:
                dependencies["modules"].append(module)
        logging.debug(f"Parsed {len(self.entity_map)} entities in {self.file_name}")
        return {"entities": self.sorted_entities(), "dependencies": {k: sorted(list(set(v))) for k, v in dependencies.items()}}

    def _extract_full_entity(self, start: int, end_header: int, content: str = None) -> str:
        """Extracts the full entity text using clean_lines for brace counting."""
        if len(self.clean_lines) <= 1:
            raise Exception("clean_lines not filled")
        content = content or self.get_clean_content()
        start_pos = start
        lines = content.splitlines()
        start_line = self.find_line(start_pos)
        start_line, end_line = self.detect_bounds(start_line, self.clean_lines)
        if start_line == end_line:
            self.parse_warn(f"Incomplete entity in file {self.file_name} at start={start}, using header end")
            return content[start:end_header]
        logging.info(f"Extracted entity from first_line={start_line} to last_line={end_line}")
        return "\n".join(self.clean_lines[start_line:end_line + 1])

SandwichPack.register_block_class(ContentCodeJs)
</python>
<python src="/lib/php_block.py" mod_time="2025-08-01 10:07:16Z">
# /lib/php_block.py, updated 2025-08-01 13:10 EEST
# Formatted with proper line breaks and indentation for project compliance.

import re
import os
import logging
from typing import Optional
from pathlib import Path
from lib.content_block import ContentBlock, estimate_tokens
from lib.sandwich_pack import SandwichPack

class ContentCodePHP(ContentBlock):
    supported_types = [".php"]

    def __init__(self, content_text: str, content_type: str, file_name: str, timestamp: str, **kwargs):
        super().__init__(content_text, content_type, file_name, timestamp, **kwargs)
        self.tag = "php"
        self.raw_quote_char = "'"
        self.open_sl_comment = ["//", "#"]
        self.entity_map = {}  # Use entity_map for consistency with Rust
        logging.debug(f"Initialized ContentCodePhp with tag={self.tag}, file_name={file_name}")

    def check_raw_escape(self, line: str, position: int, quote_char: str) -> bool:
        """Checks if the character at position is part of a PHP raw string escape sequence."""
        if position + 1 < len(line) and line[position] == self.escape_char:
            next_char = line[position + 1]
            return next_char == quote_char or next_char == self.escape_char
        return False

    def parse_content(self) -> dict:
        """Parses PHP content to extract entities and dependencies using clean_lines."""
        self.entity_map = {}
        dependencies = {"modules": [], "imports": [], "calls": []}
        clean_content = self.get_clean_content()
        lines = self.clean_lines
        content_offset = 1  # clean_content starts at line 1 in clean_lines
        class_context = None
        class_indent = None
        class_context_line = 0

        # Find classes
        class_pattern = re.compile(
            r"^(?P<indent>[ \t]*)(?:public\s+|protected\s+|private\s+)?class\s+(?P<name>\w+)\s*(?:extends\s+\w+)?\s*{",
            re.MULTILINE
        )
        for match in class_pattern.finditer(clean_content):
            start_pos = match.start('name')
            line_count = clean_content[:start_pos].count('\n')
            start_line = content_offset + line_count
            if start_line in self.entity_map:
                continue
            class_name = match.group('name')
            vis = "public"
            full_text = self._extract_full_entity(match.start(), match.end(), clean_content)
            entity = {
                "type": "class",
                "name": class_name,
                "visibility": vis,
                "file_id": self.file_id,
                "first_line": start_line,
                "tokens": estimate_tokens(full_text)
            }
            self.add_entity(start_line, entity)
            class_indent = len(match.group('indent'))
            class_context = class_name
            class_context_line = start_line
            logging.debug(f"Parsed class {class_name} at line {start_line}")

        # Find functions and methods
        fn_pattern = re.compile(
            r"^(?P<indent>[ \t]*)(?:public\s+|protected\s+|private\s+)?function\s+(?P<name>\w+)\s*\(",
            re.MULTILINE
        )
        for match in fn_pattern.finditer(clean_content):
            start_pos = match.start('name')
            line_count = clean_content[:start_pos].count('\n')
            start_line = content_offset + line_count
            if start_line in self.entity_map:
                continue
            indent = len(match.group('indent'))
            name = match.group('name')
            vis = "public"
            full_text = self._extract_full_entity(match.start(), match.end(), clean_content)
            is_method = class_context and indent > class_indent and start_line > class_context_line
            ent_type = "method" if is_method else "function"
            name_final = f"{class_context}::{name}" if is_method else name
            entity = {
                "type": ent_type,
                "name": name_final,
                "visibility": vis,
                "file_id": self.file_id,
                "first_line": start_line,
                "tokens": estimate_tokens(full_text)
            }
            self.add_entity(start_line, entity)
            logging.debug(f"Parsed {ent_type} {name_final} at line {start_line}")

        # Check class end
        for i, line in enumerate(lines[1:], 1):
            if not isinstance(line, str) or not line.strip():
                continue
            indent = len(line) - len(line.lstrip())
            if class_context and indent <= class_indent:
                class_context = None
                class_indent = None
                class_context_line = 0

        # Parse imports and calls
        import_pattern = re.compile(r"require\s*\(['\"]([^'\"]+)['\"]\)|include\s*\(['\"]([^'\"]+)['\"]\)", re.MULTILINE)
        for match in import_pattern.finditer(clean_content):
            module = match.group(1) or match.group(2)
            if module:
                dependencies["modules"].append(module)
        call_pattern = re.compile(r"\b(\w+)\s*\(")
        for match in call_pattern.finditer(clean_content):
            dependencies["calls"].append(match.group(1))
        logging.debug(f"Parsed {len(self.entity_map)} entities in {self.file_name}")
        return {"entities": self.sorted_entities(), "dependencies": {k: sorted(list(set(v))) for k, v in dependencies.items()}}

    def _extract_full_entity(self, start: int, end_header: int, content: str = None) -> str:
        """Extracts the full entity text using clean_lines for brace counting."""
        if len(self.clean_lines) <= 1:
            raise Exception("clean_lines not filled")
        content = content or self.get_clean_content()
        start_pos = start
        lines = content.splitlines()
        start_line = self.find_line(start_pos)
        start_line, end_line = self.detect_bounds(start_line, self.clean_lines)
        if start_line == end_line:
            self.parse_warn(f"Incomplete entity in file {self.file_name} at start={start}, using header end")
            return content[start:end_header]
        logging.info(f"Extracted entity from first_line={start_line} to last_line={end_line}")
        return "\n".join(self.clean_lines[start_line:end_line + 1])

SandwichPack.register_block_class(ContentCodePHP)
</python>
<python src="/lib/python_block.py" mod_time="2025-08-01 10:08:52Z">
# /lib/python_block.py, updated 2025-08-01 13:10 EEST
# Formatted with proper line breaks and indentation for project compliance.

import re
import os
import logging
from typing import Optional
from lib.content_block import ContentBlock, estimate_tokens
from lib.sandwich_pack import SandwichPack

class ContentCodePython(ContentBlock):
    supported_types = [".py"]

    def __init__(self, content_text: str, content_type: str, file_name: str, timestamp: str, **kwargs):
        super().__init__(content_text, content_type, file_name, timestamp, **kwargs)
        self.tag = "python"
        self.open_ml_string = ['"""', "'''"]
        self.close_ml_string = ['"""', "'''"]
        self.open_sl_comment = ["#"]
        self.open_ml_comment = ['"""', "'''"]
        self.close_ml_comment = ['"""', "'''"]
        self.entity_map = {}  # Use entity_map for consistency with Rust
        logging.debug(f"Initialized ContentCodePython with tag={self.tag}, file_name={file_name}")

    def detect_bounds(self, start_line, clean_lines):
        """Detects the start and end line of an entity using indentation for Python."""
        if start_line < 1 or start_line >= len(clean_lines) or not clean_lines[start_line] or not clean_lines[start_line].strip():
            logging.error(f"Invalid start line {start_line} for file {self.file_name} module [{self.module_prefix}]")
            return start_line, start_line
        initial_indent = len(clean_lines[start_line]) - len(clean_lines[start_line].lstrip())
        line_num = start_line
        while line_num < len(clean_lines):
            line = clean_lines[line_num]
            if not isinstance(line, str) or not line.strip():
                line_num += 1
                continue
            current_indent = len(line) - len(line.lstrip())
            if current_indent <= initial_indent and line_num > start_line:
                return start_line, line_num - 1
            line_num += 1
        logging.info(f"Reached end of file for entity at line {start_line} in file {self.file_name}")
        return start_line, line_num - 1

    def parse_content(self) -> dict:
        """Parses Python content to extract entities and dependencies using clean_lines."""
        self.entity_map = {}
        dependencies = {"modules": [], "imports": [], "calls": []}
        clean_content = self.get_clean_content()
        lines = self.clean_lines
        content_offset = 1  # clean_content starts at line 1 in clean_lines
        class_context = None
        class_indent = None
        class_context_line = 0

        # Find classes
        class_pattern = re.compile(
            r"^(?P<indent>[ \t]*)(?P<vis>@classmethod\s+|@staticmethod\s+)?class\s+(?P<name>\w+)\s*(?:\([^)]*\))?\s*:",
            re.MULTILINE
        )
        for match in class_pattern.finditer(clean_content):
            start_pos = match.start('name')
            line_count = clean_content[:start_pos].count('\n')
            start_line = content_offset + line_count
            if start_line in self.entity_map:
                continue
            class_name = match.group('name')
            vis = "public" if match.group('vis') else "private"
            full_text = self._extract_full_entity(match.start(), match.end(), clean_content)
            entity = {
                "type": "class",
                "name": class_name,
                "visibility": vis,
                "file_id": self.file_id,
                "first_line": start_line,
                "tokens": estimate_tokens(full_text)
            }
            self.add_entity(start_line, entity)
            class_indent = len(match.group('indent'))
            class_context = class_name
            class_context_line = start_line
            logging.debug(f"Parsed class {class_name} at line {start_line}")

        # Find functions and methods
        fn_pattern = re.compile(
            r"^(?P<indent>[ \t]*)(?P<vis>@classmethod\s+|@staticmethod\s+)?def\s+(?P<name>\w+)\s*\(",
            re.MULTILINE
        )
        for match in fn_pattern.finditer(clean_content):
            start_pos = match.start('name')
            line_count = clean_content[:start_pos].count('\n')
            start_line = content_offset + line_count
            if start_line in self.entity_map:
                continue
            indent = len(match.group('indent'))
            name = match.group('name')
            vis = "public" if match.group('vis') else "private"
            full_text = self._extract_full_entity(match.start(), match.end(), clean_content)
            is_method = class_context and indent > class_indent and start_line > class_context_line
            ent_type = "method" if is_method else "function"
            name_final = f"{class_context}::{name}" if is_method else name
            entity = {
                "type": ent_type,
                "name": name_final,
                "visibility": vis,
                "file_id": self.file_id,
                "first_line": start_line,
                "tokens": estimate_tokens(full_text)
            }
            self.add_entity(start_line, entity)
            logging.debug(f"Parsed {ent_type} {name_final} at line {start_line}")

        # Check class end
        for i, line in enumerate(lines[1:], 1):
            if not isinstance(line, str) or not line.strip():
                continue
            indent = len(line) - len(line.lstrip())
            if class_context and indent <= class_indent:
                class_context = None
                class_indent = None
                class_context_line = 0

        # Parse imports
        import_pattern = re.compile(r"from\s+([\w.]+)\s+import\s+([\w\s,]+?)(?=\s*(?:$|\n|;))", re.MULTILINE)
        for match in import_pattern.finditer(clean_content):
            items = [item.strip() for item in match.group(2).split(",")]
            for item in items:
                if item:
                    dependencies["imports"].append(item)
            dependencies["modules"].append(match.group(1))
        logging.debug(f"Parsed {len(self.entity_map)} entities in {self.file_name}")
        return {"entities": self.sorted_entities(), "dependencies": {k: sorted(list(set(v))) for k, v in dependencies.items()}}

    def _extract_full_entity(self, start: int, end_header: int, content: str = None) -> str:
        """Extracts the full entity text using clean_lines for colon counting."""
        if len(self.clean_lines) <= 1:
            raise Exception("clean_lines not filled")
        content = content or self.get_clean_content()
        start_pos = start
        lines = content.splitlines()
        start_line = self.find_line(start_pos)
        start_line, end_line = self.detect_bounds(start_line, self.clean_lines)
        if start_line == end_line:
            self.parse_warn(f"Incomplete entity in file {self.file_name} at start={start}, using header end")
            return content[start:end_header]
        logging.info(f"Extracted entity from first_line={start_line} to last_line={end_line}")
        return "\n".join(self.clean_lines[start_line:end_line + 1])

SandwichPack.register_block_class(ContentCodePython)
import re
import os
import logging
from typing import Optional
from lib.content_block import ContentBlock, estimate_tokens
from lib.sandwich_pack import SandwichPack

class ContentCodePython(ContentBlock):
    supported_types = [".py"]

    def __init__(self, content_text: str, content_type: str, file_name: str, timestamp: str, **kwargs):
        super().__init__(content_text, content_type, file_name, timestamp, **kwargs)
        self.tag = "python"
        self.open_ml_string = ['"""', "'''"]
        self.close_ml_string = ['"""', "'''"]
        self.open_sl_comment = ["#"]
        self.open_ml_comment = ['"""', "'''"]
        self.close_ml_comment = ['"""', "'''"]
        self.entity_map = {}  # Use entity_map for consistency with Rust
        logging.debug(f"Initialized ContentCodePython with tag={self.tag}, file_name={file_name}")

    def detect_bounds(self, start_line, clean_lines):
        """Detects the start and end line of an entity using indentation for Python."""
        if start_line < 1 or start_line >= len(clean_lines) or not clean_lines[start_line] or not clean_lines[start_line].strip():
            logging.error(f"Invalid start line {start_line} for file {self.file_name} module [{self.module_prefix}]")
            return start_line, start_line
        initial_indent = len(clean_lines[start_line]) - len(clean_lines[start_line].lstrip())
        line_num = start_line
        while line_num < len(clean_lines):
            line = clean_lines[line_num]
            if not isinstance(line, str) or not line.strip():
                line_num += 1
                continue
            current_indent = len(line) - len(line.lstrip())
            if current_indent <= initial_indent and line_num > start_line:
                return start_line, line_num - 1
            line_num += 1
        logging.info(f"Reached end of file for entity at line {start_line} in file {self.file_name}")
        return start_line, line_num - 1

    def parse_content(self) -> dict:
        """Parses Python content to extract entities and dependencies using clean_lines."""
        self.entity_map = {}
        dependencies = {"modules": [], "imports": [], "calls": []}
        clean_content = self.get_clean_content()
        lines = self.clean_lines
        content_offset = 1  # clean_content starts at line 1 in clean_lines
        class_context = None
        class_indent = None
        class_context_line = 0

        # Find classes
        class_pattern = re.compile(
            r"^(?P<indent>[ \t]*)(?P<vis>@classmethod\s+|@staticmethod\s+)?class\s+(?P<name>\w+)\s*(?:\([^)]*\))?\s*:",
            re.MULTILINE
        )
        for match in class_pattern.finditer(clean_content):
            start_pos = match.start('name')
            line_count = clean_content[:start_pos].count('\n')
            start_line = content_offset + line_count
            if start_line in self.entity_map:
                continue
            class_name = match.group('name')
            vis = "public" if match.group('vis') else "private"
            full_text = self._extract_full_entity(match.start(), match.end(), clean_content)
            entity = {
                "type": "class",
                "name": class_name,
                "visibility": vis,
                "file_id": self.file_id,
                "first_line": start_line,
                "tokens": estimate_tokens(full_text)
            }
            self.add_entity(start_line, entity)
            class_indent = len(match.group('indent'))
            class_context = class_name
            class_context_line = start_line
            logging.debug(f"Parsed class {class_name} at line {start_line}")

        # Find functions and methods
        fn_pattern = re.compile(
            r"^(?P<indent>[ \t]*)(?P<vis>@classmethod\s+|@staticmethod\s+)?def\s+(?P<name>\w+)\s*\(",
            re.MULTILINE
        )
        for match in fn_pattern.finditer(clean_content):
            start_pos = match.start('name')
            line_count = clean_content[:start_pos].count('\n')
            start_line = content_offset + line_count
            if start_line in self.entity_map:
                continue
            indent = len(match.group('indent'))
            name = match.group('name')
            vis = "public" if match.group('vis') else "private"
            full_text = self._extract_full_entity(match.start(), match.end(), clean_content)
            is_method = class_context and indent > class_indent and start_line > class_context_line
            ent_type = "method" if is_method else "function"
            name_final = f"{class_context}::{name}" if is_method else name
            entity = {
                "type": ent_type,
                "name": name_final,
                "visibility": vis,
                "file_id": self.file_id,
                "first_line": start_line,
                "tokens": estimate_tokens(full_text)
            }
            self.add_entity(start_line, entity)
            logging.debug(f"Parsed {ent_type} {name_final} at line {start_line}")

        # Check class end
        for i, line in enumerate(lines[1:], 1):
            if not isinstance(line, str) or not line.strip():
                continue
            indent = len(line) - len(line.lstrip())
            if class_context and indent <= class_indent:
                class_context = None
                class_indent = None
                class_context_line = 0

        # Parse imports
        import_pattern = re.compile(r"from\s+([\w.]+)\s+import\s+([\w,\s]+)", re.MULTILINE)
        for match in import_pattern.finditer(clean_content):
            items = [item.strip() for item in match.group(2).split(",")]
            for item in items:
                dependencies["imports"].append(item)
            dependencies["modules"].append(match.group(1))
        logging.debug(f"Parsed {len(self.entity_map)} entities in {self.file_name}")
        return {"entities": self.sorted_entities(), "dependencies": {k: sorted(list(set(v))) for k, v in dependencies.items()}}

    def _extract_full_entity(self, start: int, end_header: int, content: str = None) -> str:
        """Extracts the full entity text using clean_lines for colon counting."""
        if len(self.clean_lines) <= 1:
            raise Exception("clean_lines not filled")
        content = content or self.get_clean_content()
        start_pos = start
        lines = content.splitlines()
        start_line = self.find_line(start_pos)
        start_line, end_line = self.detect_bounds(start_line, self.clean_lines)
        if start_line == end_line:
            self.parse_warn(f"Incomplete entity in file {self.file_name} at start={start}, using header end")
            return content[start:end_header]
        logging.info(f"Extracted entity from first_line={start_line} to last_line={end_line}")
        return "\n".join(self.clean_lines[start_line:end_line + 1])

SandwichPack.register_block_class(ContentCodePython)
</python>
